@phdthesis{MarioRoman,
author = {{Mario Rom{\'{a}}n}},
title = {{Category Theory and Lambda Calculus}},
year = {2018}
}
@book{Spivak2014,
abstract = {Category theory was invented in the 1940s to unify and synthesize different areas in mathematics, and it has proven remarkably successful in enabling powerful communication between disparate fields and subfields within mathematics. This book shows that category theory can be useful outside of mathematics as a rigorous, flexible, and coherent modeling language throughout the sciences. Information is inherently dynamic; the same ideas can be organized and reorganized in countless ways, and the ability to translate between such organizational structures is becoming increasingly important in the sciences. Category theory offers a unifying framework for information modeling that can facilitate the translation of knowledge between disciplines. Written in an engaging and straightforward style, and assuming little background in mathematics, the book is rigorous but accessible to non-mathematicians. Using databases as an entry to category theory, it begins with sets and functions, then introduces the reader to notions that are fundamental in mathematics: monoids, groups, orders, and graphs—categories in disguise. After explaining the “big three” concepts of category theory—categories, functors, and natural transformations—the book covers other topics, including limits, colimits, functor categories, sheaves, monads, and operads. The book explains category theory by examples and exercises rather than focusing on theorems and proofs. It includes more than 300 exercises, with solutions. Category Theory for the Sciences is intended to create a bridge between the vast array of mathematical concepts used by mathematicians and the models and frameworks of such scientific disciplines as computation, neuroscience, and physics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Spivak, David},
booktitle = {arXiv preprint arXiv:1302.6946},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9780262028134},
issn = {1098-6596},
pages = {495},
pmid = {25246403},
publisher = {MIT Press},
title = {{Category theory for the sciences}},
year = {2014}
}
@book{Awodey,
author = {Awodey, Steve},
title = {{Category Theory}},
year = {2010}
}
@book{Baxter2008,
author = {Levin, Oscar},
booktitle = {Animal Genetics},
isbn = {9781534970748},
keywords = {1999,also,antigen,bola,bola-drb3,bovine,called the bovine leucocyte,complex,genotyping,includes many immune-related genes,lewin et al,mhc,of cattle,the major histocompatibility complex},
number = {5},
pages = {561--563},
title = {{Discrete Mathematics An Open Introduction}},
volume = {39},
year = {2008}
}
@article{Dalgleish2007,
author = {Hofstadter, Douglas},
journal = {Journal of Experimental Psychology: General},
keywords = {autobiographical memory,depression,executive control,overgeneral memory,working},
number = {1},
pages = {23--42},
title = {{Godel, Escher, Bach}},
volume = {136},
year = {2007}
}
@article{Cockx2017,
author = {Cockx, Jesper},
number = {June},
title = {{Dependent pattern matching and proof-relevant unification}},
year = {2017}
}
@article{Kimura2007,
author = {Kimura, Daisuke},
isbn = {978-3-540-76636-0},
issn = {03029743},
journal = {Programming Languages and Systems, Proceedings},
number = {April 2005},
pages = {415--430},
title = {{Call-by-value is dual to call-by-name, extended}},
volume = {4807},
year = {2007}
}
@article{Welzl2015,
author = {Welzl, Emo},
title = {{Geometry: Combinatorics {\&} Algorithms Lecture Notes HS 2014}},
year = {2015}
}
@misc{Abramsky,
author = {Abramsky},
title = {{Handbook of Logic in Computer Science Mathematical Structures}}
}
@article{Mokhov2017a,
abstract = {The paper presents a minimalistic and elegant approach to working with graphs in Haskell. It is built on a rigorous mathematical foun-dation — an algebra of graphs — that allows us to apply equational reasoning for proving the correctness of graph transformation al-gorithms. Algebraic graphs let us avoid partial functions typically caused by 'malformed graphs' that contain an edge referring to a non-existent vertex. This helps to liberate APIs of existing graph libraries from partial functions. The algebra of graphs can represent directed, undirected, reflex-ive and transitive graphs, as well as hypergraphs, by appropriately choosing the set of underlying axioms. The flexibility of the ap-proach is demonstrated by developing a library for constructing and transforming polymorphic graphs.},
author = {Mokhov, Andrey},
doi = {10.1145/3122955.3122956},
isbn = {9781450351829},
issn = {0362-1340},
journal = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell  - Haskell 2017},
keywords = {2017,acm reference format,algebra,algebraic graphs with class,andrey mokhov,functional pearl,graph theory,haskell,in,oxford,proceedings of 10th acm,sigplan international haskell symposium},
pages = {2--13},
title = {{Algebraic graphs with class (functional pearl)}},
url = {http://dl.acm.org/citation.cfm?doid=3122955.3122956},
year = {2017}
}
@article{Martina,
author = {Erwing, Martin},
title = {{Inductive Graphs and Functional Graph Algorithms}},
year = {2017}
}
@book{Saparbaev1988,
abstract = {A new method for obtaining the recombinant DNA based on heteroduplex-initiated site-directed insertion of alien nucleotide sequences is proposed. To generate a single-stranded region, plasmid DNA was nicked with restriction endonuclease in the presence of ethidium bromide with subsequent exonuclease III controlled digestion. The inserted DNA sequences flanked by nucleotide sequences complementary to single-stranded region were annealed with plasmid DNA and E. coli cells were transformed by the resulting heteroduplex molecules. The presented data show the possibility to insert as many as 200 nucleotides. The yield of recombinant DNA varied from 16 to 0.7{\%} as the number of nucleotides inserted correspondingly varied from 15 to 200. The site of insertion does not depend crucially on the localization of the restriction site used.},
archivePrefix = {arXiv},
arxivId = {2010 (ret. 29.4.2010)},
author = {Cormen, Thomas and Rivest, Richard and Leiserson, Charlse and Stein, Clifford},
booktitle = {Molekuliarnaia genetika, mikrobiologiia i virusologiia},
doi = {10.1163/9789004256064_hao_introduction},
eprint = {2010 (ret. 29.4.2010)},
isbn = {9780262033848},
issn = {0208-0613},
keywords = {Base Sequence,Cloning,DNA,DNA Restriction Enzymes,Escherichia coli,Escherichia coli: genetics,Molecular,Nucleic Acid Heteroduplexes,Nucleic Acid Hybridization,Plasmids,Recombinant},
number = {2},
pages = {12--6},
pmid = {2836723},
title = {{Introduction to Algorithms}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2560149&tool=pmcentrez&rendertype=abstract},
year = {1988}
}
@book{Leary,
author = {Leary, Christopher C and Kristiansen, Lars},
isbn = {9781942341079},
title = {{A Friendly Introduction to Mathematical Logic}},
year = {2015}
}
@article{Castillo,
author = {Castillo, Carlos Ivorra},
journal = {2017},
title = {{Logica Matematica}}
}
@article{Strecker2004,
author = {Strecker, George E},
title = {{Concrete Categories: The Joy of Cats}},
year = {2004}
}
@article{Setzer2004,
author = {Setzer, Anton},
keywords = {definitions,generic programming,inductive-recursive,kleene,kripke-platek set theory,mahlo universe,martin-l,of type theory,proof theory,s o,w-type,well-founded trees},
pages = {1--35},
title = {{Proof Theory of Martin-L{\"{o}}f Type Theory – An Overview The Notion of Proof-Theoretic Strength}},
year = {2004}
}
@article{Wadler2014,
author = {Wadler, Philip},
doi = {10.1017/S095679681400001X},
number = {January},
pages = {384--418},
title = {{Propositions as sessions}},
volume = {24},
year = {2014}
}
@article{Lawvere,
author = {Lawvere, F William and Schanuel, Stephen H},
title = {{Conceptual Mathematics: A first introduction to categories}}
}
@article{Speight2018,
author = {Speight, Sam},
number = {January},
title = {{Impredicative Encodings of Inductive Types in Homotopy Type Theory}},
year = {2018}
}
@article{Mynhardt2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1504.00617v1},
author = {Mynhardt, C M Kieka and Bommel, Christopher M Van and Vw, Canada},
eprint = {arXiv:1504.00617v1},
keywords = {05c10,05c70,ams subject classification 2010,planar graphs,rational triangle decompositions,triangle decompositions},
pages = {1--14},
title = {{Triangle Decompositions of Planar Graphs}},
year = {2015}
}
@article{Kahle2014,
author = {Kahle, Reinhard},
doi = {10.1007/s10516-014-9252-9},
issn = {1122-1151},
journal = {Axiomathes},
keywords = {big proofs,proof {\'{a}} mathematical proof,{\'{a}} computer-assisted proof {\'{a}}},
pages = {79--91},
title = {{What is a Proof?}},
url = {http://link.springer.com/10.1007/s10516-014-9252-9},
year = {2014}
}
@article{Micinski2012,
abstract = {We develop a calculus for lazy functional programming based on recursion operators associated with data type definitions. For these operators we derive various algebraic laws that are useful in deriving and manipulating programs. We shall show that all example functions in Bird and Wadler's "Introduction to Functional Programming" can be expressed using these operators.},
author = {Micinski, Kristopher},
pages = {2012},
title = {{Functional Programming with Bananas , Lenses , Envelopes and Barbed Wire}},
year = {2012}
}
@misc{Kunen2012,
abstract = {Studies in Logic Mathematical Logic and Foundations},
author = {Kunen, Kenneth},
title = {{The Foundations of Mathematics}},
year = {2012}
}
@book{Papadimitriou,
author = {Papadimitriou, Christos H. and Steiglitz, Kenneth},
title = {{Combinatorial Optimization: Algorithms and Complexity}},
year = {1998}
}
@article{Geuvers2013,
author = {Geuvers, Herman},
number = {November},
pages = {5--7},
title = {{Pure Type Systems}},
year = {2013}
}
@article{Avigad2010a,
author = {Avigad, Jeremy},
doi = {10.1093/acprof:oso/9780199296453.003.0013},
isbn = {9780191711961},
issn = {14672987},
journal = {The Philosophy of Mathematical Practice},
keywords = {Formal verification,Highlevel inferences,Informal speech,Mathematical proofs,Mathematical understanding},
number = {May 2005},
title = {{Understanding Proofs}},
year = {2010}
}
@article{Avigad2010,
author = {Avigad, Jeremy},
journal = {Journal of Indian Council of Philosophical {\ldots}},
pages = {1--27},
title = {{Understanding, formal verification, and the philosophy of mathematics}},
url = {http://www.andrew.cmu.edu/user/avigad/Papers/understanding2.pdf},
year = {2010}
}
@article{La2006,
author = {Castillo, Carlos Ivorra},
pages = {521},
title = {{Topologia Algebraica}},
year = {2006}
}
@article{Tutte1963,
abstract = {W. T. Tutte published a paper in 1963 entitled {\{}``How{\}} to Draw a Graph''. Tutte's motivation was mathematical, and his paper can be seen as a contribution to the long tradition of geometric representations of combinatorial objects. Over the following 40 odd years, the motivation for creating visual representations of graphs has changed from mathematical curiosity to Visual Analytics. Current demand for Graph Drawing methods is now high, because of the potential for more human-comprehensible visual forms in industries as diverse as Biotechnology, Homeland Security, and Sensor Networks. Many new methods have been proposed, tested, implemented, and found their way into commercial tools. This paper describes two strands of this history: the force directed approach, and the planarity approach. Both approaches originate in Tutte's paper.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Tutte, W. T.},
doi = {10.1112/plms/s3-13.1.743},
eprint = {9809069v1},
isbn = {978-1-4471-2803-8},
issn = {1460244X},
journal = {Proceedings of the London Mathematical Society},
number = {1},
pages = {743--767},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{How to Draw a Graph}},
url = {http://doi.wiley.com/10.1112/plms/s3-13.1.743},
volume = {s3-13},
year = {1963}
}
@article{Holt1997a,
author = {Holt, Robert M.},
isbn = {0-201-41667-0},
number = {August},
title = {{Conceptual Model for Transport Processes in the Culebra Dolomite Member, Rustler Formation Sandia National Laboratories}},
year = {1997}
}
@article{Awodey2012,
abstract = {The purpose of this survey article is to introduce the reader to a connection between Logic, Geometry, and Algebra which has recently come to light in the form of an interpretation of the constructive type theory of Martin-L$\backslash$"of into homotopy theory, resulting in new examples of higher-dimensional categories.},
archivePrefix = {arXiv},
arxivId = {1010.1810},
author = {Awodey, Steve},
doi = {10.1007/978-94-007-4435-6_9},
eprint = {1010.1810},
isbn = {9789400744356},
issn = {16113349},
journal = {Epistemology versus Ontology: Essays on the Philosophy and Foundations of Mathematics in Honour of per Martin-Lof},
pages = {183--201},
title = {{Type theory and homotopy}},
year = {2012}
}
@article{Rojas2015,
abstract = {This paper is a concise and painless introduction to the {\$}\backslashlambda{\$}-calculus. This formalism was developed by Alonzo Church as a tool for studying the mathematical properties of effectively computable functions. The formalism became popular and has provided a strong theoretical foundation for the family of functional programming languages. This tutorial shows how to perform arithmetical and logical computations using the {\$}\backslashlambda{\$}-calculus and how to define recursive functions, even though {\$}\backslashlambda{\$}-calculus functions are unnamed and thus cannot refer explicitly to themselves.},
archivePrefix = {arXiv},
arxivId = {1503.09060},
author = {Rojas, Raul},
doi = {10.1006/anbe.1999.1219},
eprint = {1503.09060},
isbn = {9788578110796},
issn = {00033472},
pages = {1--9},
pmid = {10512656},
title = {{A Tutorial Introduction to the Lambda Calculus}},
url = {http://arxiv.org/abs/1503.09060},
year = {2015}
}
@article{Erb2016,
author = {Erb, Craig and Carter, Nathan and Mcmichael, Wes and Samuel, Selva and Krueger, Dave},
title = {{Forall X}},
year = {2016}
}
@article{Groups1983,
author = {Vince, Andrew},
journal = {Journal of Combinatorial Theory},
pages = {1--21},
title = {{Combinatorial Maps}},
volume = {21},
year = {1981}
}
@book{Laaksonen2017,
abstract = {The purpose of this book is to give the reader a thorough introduction to competitive programming. The book is especially intended for students who want to learn algorithms and possibly participate in the International Olympiad in Informatics (IOI) or in the International Collegiate Programming Contest (ICPC).},
author = {Laaksonen, Antti},
title = {{Competitive Programmer's Handbook}},
year = {2017}
}
@article{Bradley2018,
abstract = {This is a collection of introductory, expository notes on applied category theory, inspired by the 2018 Applied Category Theory Workshop, and in these notes we take a leisurely stroll through two themes (functorial semantics and compositionality), two constructions (monoidal categories and decorated cospans) and two examples (chemical reaction networks and natural language processing) within the field.},
archivePrefix = {arXiv},
arxivId = {1809.05923},
author = {Bradley, Tai-Danae},
eprint = {1809.05923},
isbn = {9783540710929 (pbk.)},
title = {{What is Applied Category Theory?}},
url = {http://arxiv.org/abs/1809.05923},
year = {2018}
}
@article{Mycroft2016,
author = {Mycroft, Alan},
title = {{Introduction Hoare Logic and Model Checking}},
year = {2016}
}
@article{Cohen2016,
abstract = {This paper presents a type theory in which it is possible to directly manipulate {\$}n{\$}-dimensional cubes (points, lines, squares, cubes, etc.) based on an interpretation of dependent type theory in a cubical set model. This enables new ways to reason about identity types, for instance, function extensionality is directly provable in the system. Further, Voevodsky's univalence axiom is provable in this system. We also explain an extension with some higher inductive types like the circle and propositional truncation. Finally we provide semantics for this cubical type theory in a constructive meta-theory.},
archivePrefix = {arXiv},
arxivId = {1611.02108},
author = {Cohen, Cyril and Coquand, Thierry and Huber, Simon and M{\"{o}}rtberg, Anders},
doi = {10.4230/LIPIcs.TYPES.2015.5},
eprint = {1611.02108},
isbn = {978-3-95977-030-9},
issn = {1868-8969},
pages = {1--34},
title = {{Cubical Type Theory: a constructive interpretation of the univalence axiom}},
url = {http://arxiv.org/abs/1611.02108},
year = {2016}
}
@article{Bergsten2017,
author = {Bergsten, Erik},
number = {June},
title = {{Methods for using Agda to prove Safety and Liveness for Concurrent Programs}},
year = {2017}
}
@article{Tye2003,
author = {of Philosophy, Stanford Encyclopedia},
journal = {Www.Plato.Stanford.Edu/Entries/Qualia},
title = {{Constructive Mathematics}},
year = {2003}
}
@article{AlanHajek2017,
abstract = {Title from title screen.},
author = {{Stanford Encyclopedia of Philosophy}},
doi = {10.1111/1467-9973.00225},
isbn = {0-87548-354-2},
issn = {1095-5054},
number = {September},
pages = {1--22},
title = {{Type Theory}},
url = {https://plato.stanford.edu/entries/computational-complexity/},
year = {2017}
}
@article{Logic2009,
author = {of Philosophy, Stanford Encyclopedia},
title = {{Second-order and Higher-order Logic}},
year = {2009}
}
@book{Raynal2013,
abstract = {The advent of new architectures and computing platforms means that synchronization and concurrent computing are among the most important topics in computing science. Concurrent programs are made up of cooperating entities -- processors, processes, agents, peers, sensors -- and synchronization is the set of concepts, rules and mechanisms that allow them to coordinate their local computations in order to realize a common task. This book is devoted to the most difficult part of concurrent programming, namely synchronization concepts, techniques and principles when the cooperating entities are asynchronous, communicate through a shared memory, and may experience failures. Synchronization is no longer a set of tricks but, due to research results in recent decades, it relies today on sane scientific foundations as explained in this book. In this book the author explains synchronization and the implementation of concurrent objects, presenting in a uniform and comprehensive way the major theoretical and practical results of the past 30 years. Among the key features of the book are a new look at lock-based synchronization (mutual exclusion, semaphores, monitors, path expressions); an introduction to the atomicity consistency criterion and its properties and a specific chapter on transactional memory; an introduction to mutex-freedom and associated progress conditions such as obstruction-freedom and wait-freedom; a presentation of Lamport's hierarchy of safe, regular and atomic registers and associated wait-free constructions; a description of numerous wait-free constructions of concurrent objects (queues, stacks, weak counters, snapshot objects, renaming objects, etc.); a presentation of the computability power of concurrent objects including the notions of universal construction, consensus number and the associated Herlihy's hierarchy; and a survey of failure detector-based constructions of consensus objects. The book is suitable for advanced undergraduate students and graduate students in computer science or computer engineering, graduate students in mathematics interested in the foundations of process synchronization, and practitioners and engineers who need to produce correct concurrent software. The reader should have a basic knowledge of algorithms and operating systems.},
author = {Raynal, Michel},
booktitle = {Concurrent Programming: Algorithms, Principles, and Foundations},
doi = {10.1007/978-3-642-32027-9},
isbn = {9783642320279},
pages = {1--515},
title = {{Concurrent programming: Algorithms, principles, and foundations}},
volume = {9783642320},
year = {2013}
}
@phdthesis{proof-by-rewriting,
abstract = {It is very common to hear that pen-and-paper mathematics run smoother than computer-aided reason- ing. The biggest pointed problem being the overhead of information one has to provide a computer to be able to perform simple reasoning, with the upside being the absolute certainty that one is cor- rect. One such tool for computer reasoning is Agda, a dependently typed, pure, functional language. We explore this language in its two fronts: a proof-assistant and a programming language. On the proof-assistant side, we have developed a library for Relational Algebraic reasoning, which handles a big portion of the relational calculus, including a prototype of generic catamorphisms. However, the overhead of code is still present when reasoning with our library. We tackle this by automating the context inference of substitutions, based on the reflection mechanism of Agda. Our tactics allow one to perform rewrites just like on pen-and-paper mathematics. Several tactics, some of which are experimental generalizations of simpler variants, are presented with a thorough discussion on how do they work. The full source code corresponding to this dissertation is available on the internet},
author = {Miraldo, Victor Cacciari},
keywords = {agda},
mendeley-tags = {agda},
number = {June},
title = {{Proof by Rewriting in Agda}},
year = {2015}
}
@book{Paleo,
author = {Paleo, BrunoWoltzenlogel},
title = {{Encyclopedia of Proof Systems}},
year = {2018}
}
@article{Seldin2008,
author = {Seldin, Jonathan P},
pages = {1--24},
title = {{On the relation between Church-style typing and Curry-style typing ∗}},
year = {2008}
}
@article{Doorn2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1808.10690v1},
author = {Doorn, Floris Van and Avigad, Jeremy and Shulman, Mike},
eprint = {arXiv:1808.10690v1},
number = {May},
title = {{On the Formalization of Higher Inductive Types and Synthetic Homotopy Theory}},
year = {2018}
}
@article{ColindeVerdiere,
archivePrefix = {arXiv},
arxivId = {arXiv:1702.05358v2},
author = {{Colin de Verdi{\`{e}}re}, {\'{E}}ric},
eprint = {arXiv:1702.05358v2},
title = {{Computational Topology of Graphs on Surfaces}},
year = {2017}
}
@misc{hits,
author = {van der Weide, Niels},
keywords = {hits,hott},
mendeley-tags = {hits,hott},
publisher = {RADBOUD UNIVERSITY},
title = {{Higher Inductive Types}},
url = {http://arxiv.org/abs/1803.07032%0Ahttps://arxiv.org/abs/1803.07032},
year = {2016}
}
@article{DeVerdiere2018,
abstract = {We consider the problem of deciding whether an input graph G admits a topological embedding into a two-dimensional simplicial complex C. This problem includes, among others, the embeddability problem of a graph on a surface and the topological crossing number of a graph, but is more general. The problem is NP-complete when C is part of the input, and we give a polynomial-time algorithm if the complex C is fixed. Our strategy is to reduce the problem to an embedding extension problem on a surface, which has the following form: Given a subgraph H' of a graph G', and an embedding of H' on a surface S, can that embedding be extended to an embedding of G' on S? Such problems can be solved, in turn, using a key component in Mohar's algorithm to decide the embeddability of a graph on a fixed surface (STOC 1996, SIAM J. Discr. Math. 1999).},
archivePrefix = {arXiv},
arxivId = {1803.07032},
author = {{Colin de Verdi{\`{e}}re}, {\'{E}}ric and Magnard, Thomas and Mohar, Bojan},
doi = {10.4230/LIPIcs.SoCG.2018.27},
eprint = {1803.07032},
isbn = {9783959770668},
issn = {18688969},
keywords = {and phrases computational topology,and while the third,anr-17-ce40-0033 of the french,are supported by grant,author was invited professor,embedding,france,funding part of this,graph,infor-,matique,national,paris,simplicial complex,surface,the first two authors,there,were at d{\'{e}}partement d,work was done while,{\'{e}}cole normale sup{\'{e}}rieure},
number = {SoCG 2018},
title = {{Embedding graphs into two-dimensional simplicial complexes}},
url = {http://arxiv.org/abs/1803.07032%0Ahttps://arxiv.org/abs/1803.07032},
year = {2018}
}
@article{ColindeVerdiere2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1310.2745v1},
author = {{Colin de Verdi{\`{e}}re}, {\'{E}}ric and de Mesmay, Arnaud},
doi = {10.1007/s00454-013-9555-4},
eprint = {arXiv:1310.2745v1},
isbn = {9781450312998},
issn = {01795376},
journal = {Discrete and Computational Geometry},
keywords = {Computational topology,Embedded graph,Homotopy,Isotopy,Topological graph theory},
number = {1},
pages = {171--206},
title = {{Testing Graph Isotopy on Surfaces}},
volume = {51},
year = {2014}
}
@misc{MacLane,
author = {MacLane, Saunders},
title = {{A combinatorial condition for planar graphs}},
year = {1937}
}
@article{Wadler2003,
author = {Wadler, Philip},
isbn = {1581137567},
keywords = {curry-howard correspondence,de morgan dual,duction,lambda,lambda calculus,logic,natural de-,sequent calculus},
number = {1935},
title = {{Call-by-Value is Dual to Call-by-Name}},
year = {2003}
}
@article{Borradaile2016,
abstract = {We study the problems of finding a minimum cycle basis (a minimum weight set of cycles that form a basis for the cycle space) and a minimum homology basis (a minimum weight set of cycles that generates the {\$}1{\$}-dimensional ({\$}\backslashmathbb{\{}Z{\}}{\_}2{\$})-homology classes) of an undirected graph embedded on a surface. The problems are closely related, because the minimum cycle basis of a graph contains its minimum homology basis, and the minimum homology basis of the {\$}1{\$}-skeleton of any graph is exactly its minimum cycle basis. For the minimum cycle basis problem, we give a deterministic {\$}O(n{\^{}}\backslashomega+2{\^{}}{\{}2g{\}}n{\^{}}2+m){\$}-time algorithm for graphs embedded on an orientable surface of genus {\$}g{\$}. The best known existing algorithms for surface embedded graphs are those for general graphs: an {\$}O(m{\^{}}\backslashomega){\$} time Monte Carlo algorithm and a deterministic {\$}O(nm{\^{}}2/\backslashlog n + n{\^{}}2 m){\$} time algorithm. For the minimum homology basis problem, we give a deterministic {\$}O((g+b){\^{}}3 n \backslashlog n + m){\$}-time algorithm for graphs embedded on an orientable or non-orientable surface of genus {\$}g{\$} with {\$}b{\$} boundary components, assuming shortest paths are unique, improving on existing algorithms for many values of {\$}g{\$} and {\$}n{\$}. The assumption of unique shortest paths can be avoided with high probability using randomization or deterministically by increasing the running time of the homology basis algorithm by a factor of {\$}O(\backslashlog n){\$}.},
archivePrefix = {arXiv},
arxivId = {1607.05112},
author = {Borradaile, Glencora and Chambers, Erin Wolf and Fox, Kyle and Nayyeri, Amir},
doi = {10.4230/LIPIcs.SoCG.2016.23},
eprint = {1607.05112},
isbn = {9783959770095},
issn = {18688969},
title = {{Minimum cycle and homology bases of surface embedded graphs}},
url = {http://arxiv.org/abs/1607.05112},
year = {2016}
}
@article{Dasgupta2006,
abstract = {This text, extensively class-tested over a decade at UC Berkeley and UC San Diego, explains the fundamentals of algorithms in a story line that makes the material enjoyable and easy to digest. Emphasis is placed on understanding the crisp mathematical idea behind each},
author = {Dasgupta, Sanjoy and Papadimitriou, Christos H.},
isbn = {9780070636613},
journal = {Book},
title = {{Algorithms}},
year = {2006}
}
@article{According,
author = {According, Introduction and Dictionary, Oxford English and Cd-rom, O E D I I and Sos, Using},
title = {{Handbook of Process Algebra}},
year = {2000}
}
@book{Marlow2013,
abstract = {Haskell provides a rich set of abstractions for parallel and concurrent programming. This tutorial covers the basic concepts involved in writing parallel and concurrent programs in Haskell, and takes a deliberately practical approach: most of the examples are real Haskell programs that you can compile, run, measure, modify and experiment with. We cover parallel programming with the @Eval@ monad, Evaluation Strategies, and the @Par@ monad. On the concurrent side, we cover threads, @MVar@s, asynchronous exceptions, Software Transactional Memory, the Foreign Function Interface, and briefly look at the construction of high-speed network servers in Haskell.},
author = {Marlow, Simon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
isbn = {978-1-4493-3594-6},
issn = {03029743},
pages = {322},
title = {{Parallel and Concurrent Programming in Haskell: Techniques for Multicore and Multithreaded Programming}},
url = {http://chimera.labs.oreilly.com/books/1230000000929},
volume = {7241 LNCS},
year = {2013}
}
@article{Hinze2005,
abstract = {This pearl explains Church numerals, twice. The first explanation links Church numerals to Peano numerals via the well-known encoding of data types in the polymorphic lambda-calculus. This view suggests that Church numerals are folds in disguise. The second explanation, which is more elaborate, but also more insightful, derives Church numerals from first principles, that is, from an algebraic specification of addition and multiplication. Additionally, we illustrate the use of the parametricity theorem by proving exponentiation as reverse application correct.},
author = {Hinze, Ralf},
doi = {10.1017/S0956796804005313},
issn = {09567968},
journal = {Journal of Functional Programming},
number = {1},
pages = {1--13},
title = {{Church numerals, twice!}},
volume = {15},
year = {2005}
}
@article{Abel2018,
author = {Abel, Andreas and Devriese, Dominique and Cockx, Jesper and Wadler, Philip and Timany, Amin},
number = {May},
title = {{Leibniz Equality is Isomorphic to Martin-L{\"{o}}f Identity , Parametrically}},
year = {2018}
}
@article{Zalakain2018,
author = {Zalakain, Uma},
keywords = {agda,monoids,rings},
mendeley-tags = {agda,monoids,rings},
pages = {2017--2018},
title = {{Evidence-providing problem solvers in Agda}},
year = {2018}
}
@online{hott-in:agda,
author = {Brunerie, Guillaume and {Hou (Favonia)}, Kuen-Bang and Cavallo, Evan and Finster, Eric and Cockx, Jesper and Sattler, Christian and Jeris, Chris and Shulman, Michael and Others},
title = {{Homotopy Type Theory in Agda}},
url = {https://github.com/HoTT/HoTT-Agda},
year = {2018}
}
@article{Bove2009,
abstract = {In these lecture notes we give an introduction to functional programming with dependent types. We use the dependently typed programming language Agda which is based on ideas in Martin-L{\"{o}}f type theory and Martin-L{\"{o}}fs logical framework. We begin by showing how to do simply typed functional programming, and discuss the differences between Agdas type system and the Hindley-Milner type system, which underlies mainstream typed functional programming languages like Haskell and ML. We then show how to employ dependent types for programming with functional data structures such as vectors and binary search trees. We go on to explain the Curry-Howard identification of propositions and types, and how it makes Agda not only a programming language but also a programming logic. According to Curry-Howard, we also identify programs and proofs, something which is possible only by requiring that all program terminate. However, we show in the final section a method for encoding general and possibly partial recursive functions as total functions using dependent types.},
author = {Bove, Ana and Dybjer, Peter},
doi = {10.1007/978-3-642-03153-3_2},
isbn = {3642031528},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {57--99},
title = {{Dependent types at work}},
volume = {5520 LNCS},
year = {2009}
}
@article{McBride2004,
abstract = {Research in dependent type theories [ML71a] has, in the past, concentrated on its use in the presentation of theorems and theorem-proving. This thesis is concerned mainly with the exploitation of the computational aspects of type theory for programming, in a context where the properties of programs may readily be specified and established. In particular, it develops technology for programming with dependent inductive families of datatypes and proving those programs correct. It demonstrates the considerable advantage to be gained by indexing data structures with pertinent characteristic information whose soundness is ensured by typechecking, rather than human effort. Type theory traditionally presents safe and terminating computation on inductive datatypes by means of elimination rules which serve as induction principles and, via their associated reduction behaviour, recursion operators [Dyb91]. In the programming language arena, these appear somewhat cumbersome and give rise to unappealing code, complicated by the inevitable interaction between case analysis on dependent types and equational reasoning on their indices which must appear explicitly in the terms. Thierry Coquand's proposal [Coq92] to equip type theory directly with the kind of pattern matching notation to which functional programmers have become used over the past three decades [Bur69, McB70] offers a remedy to many of these difficulties. However, the status of pattern matching relative to the traditional elimination rules has until now been in doubt. Pattern matching implies the uniqueness of identity proofs, which Martin Hofmann showed underivable from the conventional definition of equality [Hof95]. This thesis shows that the adoption of this uniqueness as axiomatic is sufficient t make pattern matching admissible. A datatype's elimination rule allows abstraction only over the whole inductively defined family. In order to support pattern matching, the application of such rules to specific instances of dependent families has been systematised. The underlying analysis extends beyond datatypes to other rules of a similar second order character, suggesting they may have other roles to play in the specification, verification and, perhaps, derivation of programs. The technique developed shifts the specificity from the instantiation of the type's indices into equational constraints on indices freely chosen, allowing the elimination rule to be applied. Elimination by this means leaves equational hypothesis in the resulting subgoals, which must be solved if further progress is to be made. The first-order unification algorithm for constructor forms in simple types presented in [McB96] has been extended to cover dependent datatypes as well, yielding completely automated solution to a class of problems which can be syntactically defined. The justification and operation of these techniques requires the machine to construct and exploit a standardised collection of auxiliary lemmas for each datatype. This is greatly facilitated by two technical developments of interest in their own right: a more convenient definition of equality, with a relaxed formulation rule allowing elements of different types to be compared, but nonetheless equivalent to the usual equality plus the axiom of uniqueness; a type theory, OLEG, which incorporates incomplete objects, accounting for their `holes' entirely within the typing judgments and, novelly, not requiring any notion of explicit substitution to manage their scopes. A substantial prototype has been implemented, extending the proof assistant LEGO [LP92]. A number of programs are developed by way of example. Chiefly, the increased expressivity of dependent datatypes is shown to capture a standard first-order unification algorithm within the class of structurally recursive programs, removing any need for a termination argument. Furthermore, the use of elimination rules in specifying the components of the program simplifies significantly its correctness proof.},
author = {McBride, Conor},
title = {{Dependently Typed Functional Programs and their Proofs}},
url = {http://hdl.handle.net/1842/374},
year = {2004}
}
@article{Licata2015,
author = {Licata, Daniel R. and Brunerie, Guillaume},
doi = {10.1109/LICS.2015.19},
isbn = {9781479988754},
issn = {10436871},
journal = {Proceedings - Symposium on Logic in Computer Science},
keywords = {Homotopy type theory,Type theory},
pages = {92--103},
title = {{A cubical approach to synthetic homotopy theory}},
volume = {2015-July},
year = {2015}
}
@article{Nieuwenhuis2006,
author = {Nieuwenhuis, Robert and Nieuwenhuis, Robert and Oliveras, Albert and Oliveras, Albert and Tinelli, Cesare and Tinelli, Cesare},
doi = {10.1145/1217856.1217859},
issn = {0004-5411},
journal = {J. Acm},
number = {6},
pages = {937--977},
title = {{Solving SAT and SAT Modulo Theories: From an Abstract Davis–Putnam–Logemann–Loveland Procedure to DPLL(T)}},
volume = {53},
year = {2006}
}
@article{Carlstrom2013,
author = {Carlstr{\~{o}}m, Jesper},
title = {{Logic}},
volume = {2008},
year = {2013}
}
@article{Licata2013,
abstract = {Recent work on homotopy type theory exploits an exciting new correspondence between Martin-Lof's dependent type theory and the mathematical disciplines of category theory and homotopy theory. The category theory and homotopy theory suggest new principles to add to type theory, and type theory can be used in novel ways to formalize these areas of mathematics. In this paper, we formalize a basic result in algebraic topology, that the fundamental group of the circle is the integers. Though simple, this example is interesting for several reasons: it illustrates the new principles in homotopy type theory; it mixes ideas from traditional homotopy-theoretic proofs of the result with type-theoretic inductive reasoning; and it provides a context for understanding an existing puzzle in type theory---that a universe (type of types) is necessary to prove that the constructors of inductive types are disjoint and injective.},
archivePrefix = {arXiv},
arxivId = {1301.3443},
author = {Licata, Daniel R. and Shulman, Michael},
doi = {10.1109/LICS.2013.28},
eprint = {1301.3443},
isbn = {978-1-4799-0413-6},
issn = {10436871},
journal = {Proceedings - Symposium on Logic in Computer Science},
pages = {223--232},
title = {{Calculating the fundamental group of the circle in homotopy type theory}},
year = {2013}
}
@book{peter,
author = {{Peter Jan Pahl}, Rudolf Damrath},
title = {{Mathematical Foundations of Computational Engineering: A Handbook}},
type = {pdf}
}
@article{Hanks2011,
abstract = {Connecting mathematical logic and computation, it ensures that some aspects of programming are absolute.},
author = {Hanks, Peter W.},
doi = {10.1093/mind/fzr011},
journal = {Mind},
number = {477},
pages = {11--52},
title = {{Structured propositions as types}},
volume = {120},
year = {2011}
}
@article{Wadler2015PT,
address = {New York, NY, USA},
author = {Wadler, Philip},
doi = {10.1145/2699407},
issn = {0001-0782},
journal = {Commun. ACM},
number = {12},
pages = {75--84},
publisher = {ACM},
title = {{Propositions As Types}},
url = {http://doi.acm.org/10.1145/2699407},
volume = {58},
year = {2015}
}
@article{Mokhov2014,
author = {Mokhov, Andrey and Khomenko, Victor},
doi = {10.1145/2627351},
issn = {15399087},
journal = {ACM Transactions on Embedded Computing Systems},
keywords = {Parameterised graphs,conditional partial-order graphs,instruction set architecture,microelectronics,switching networks,synthesis,transistor networks},
number = {4s},
pages = {1--22},
publisher = {ACM},
title = {{Algebra of Parameterised Graphs}},
url = {http://dl.acm.org/citation.cfm?doid=2601432.2627351},
volume = {13},
year = {2014}
}
@inproceedings{Mokhov2017,
address = {New York, New York, USA},
author = {Mokhov, Andrey and Andrey and Mokhov and Andrey},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell  - Haskell 2017},
doi = {10.1145/3122955.3122956},
isbn = {9781450351829},
issn = {0362-1340},
keywords = {Haskell,algebra,graph theory},
number = {10},
pages = {2--13},
publisher = {ACM Press},
title = {{Algebraic graphs with class (functional pearl)}},
url = {http://dl.acm.org/citation.cfm?doid=3122955.3122956},
volume = {52},
year = {2017}
}
@article{Hong,
author = {Hong, Seok-hee and Kaufmann, Michael and Kobourov, Stephen G},
keywords = {and phrases graph drawing,combinatorial geometry,geometric algorithms,graph algorithms,graph theory,visualization},
number = {11},
pages = {35--62},
title = {{Beyond-Planar Graphs : Algorithmics and Combinatorics Edited by}},
volume = {6}
}
@article{Baur2012,
author = {Baur, Melanie},
title = {{Combinatorial Concepts and Algorithms for Drawing Planar Graphs}},
year = {2012}
}
@misc{Kuratowski1930,
author = {Kuratowski, Casimir},
booktitle = {Fundamenta Mathematicae},
doi = {10.4064/fm-15-1-271-283},
issn = {0016-2736},
pages = {271--283},
title = {{Sur le probl{\{}{\`{e}}{\}}me des courbes gauches en Topologie}},
volume = {15},
year = {1930}
}
@article{Ahrens,
abstract = {We prove a conjecture about the constructibility of coinductive types—in the principled form of indexed M-types—in Homotopy Type Theory. The conjecture says that in the presence of inductive types, coinductive types are derivable. Indeed, in this work, we construct coinductive types in a subsystem of Homotopy Type Theory; this subsystem is given by Intensional Martin-L{\"{o}}f type theory with natural numbers and Voevodsky's Univalence Axiom. Our results are mechanized in the computer proof assistant Agda.},
archivePrefix = {arXiv},
arxivId = {1504.02949},
author = {Ahrens, Benedikt and Capriotti, Paolo and Spadotti, R{\'{e}}gis},
doi = {10.4230/LIPIcs.TLCA.2015.17},
eprint = {1504.02949},
isbn = {9783939897873},
issn = {18688969},
keywords = {1998 ACM,Classification F32,Languages,Programming,Semantics of,Subject},
number = {1},
pages = {1--12},
title = {{Non-wellfounded trees in Homotopy Type Theory *}}
}
@article{Brunerie2016,
abstract = {The goal of this thesis is to prove that {\$}\backslashpi{\_}4(S{\^{}}3) \backslashsimeq \backslashmathbb{\{}Z{\}}/2\backslashmathbb{\{}Z{\}}{\$} in homotopy type theory. In particular it is a constructive and purely homotopy-theoretic proof. We first recall the basic concepts of homotopy type theory, and we prove some well-known results about the homotopy groups of spheres: the computation of the homotopy groups of the circle, the triviality of those of the form {\$}\backslashpi{\_}k(S{\^{}}n){\$} with {\$}k {\textless} n{\$}, and the construction of the Hopf fibration. We then move to more advanced tools. In particular, we define the James construction which allows us to prove the Freudenthal suspension theorem and the fact that there exists a natural number {\$}n{\$} such that {\$}\backslashpi{\_}4(S{\^{}}3) \backslashsimeq \backslashmathbb{\{}Z{\}}/n\backslashmathbb{\{}Z{\}}{\$}. Then we study the smash product of spheres, we construct the cohomology ring of a space, and we introduce the Hopf invariant, allowing us to narrow down the {\$}n{\$} to either {\$}1{\$} or {\$}2{\$}. The Hopf invariant also allows us to prove that all the groups of the form {\$}\backslashpi{\_}{\{}4n-1{\}}(S{\^{}}{\{}2n{\}}){\$} are infinite. Finally we construct the Gysin exact sequence, allowing us to compute the cohomology of {\$}\backslashmathbb{\{}C{\}}P{\^{}}2{\$} and to prove that {\$}\backslashpi{\_}4(S{\^{}}3) \backslashsimeq \backslashmathbb{\{}Z{\}}/2\backslashmathbb{\{}Z{\}}{\$} and that more generally {\$}\backslashpi{\_}{\{}n+1{\}}(S{\^{}}n) \backslashsimeq \backslashmathbb{\{}Z{\}}/2\backslashmathbb{\{}Z{\}}{\$} for every {\$}n \backslashge 3{\$}.},
archivePrefix = {arXiv},
arxivId = {1606.05916},
author = {Brunerie, Guillaume},
eprint = {1606.05916},
title = {{On the homotopy groups of spheres in homotopy type theory}},
url = {http://arxiv.org/abs/1606.05916},
year = {2016}
}
@article{Frumin,
abstract = {We study different formalizations of finite sets in homotopy type theory to obtain a general definition that exhibits both the computational facilities and the proof principles expected from finite sets. We use higher inductive types to define the type K(A) of " finite sets over type A " {\`{a}} la Kuratowski without assuming A has decidable equality. We show how to define basic functions and prove basic properties, and then we give two applications of our definition. On the foundational side, we use K to define the no-tions of " Kuratowski-finite type " and " Kuratowski-finite subobject " which we contrast with established notions, e.g., Bishop-finite types and enumerated types. We argue that Kuratowski-finiteness is the most general and flexible one of those, and we define the usual operations on finite types and subobjects. From the computational perspective, we show how to use K(A) for an abstract interface for well-known finite set im-plementations such as tree-and list-like data structures. This implies that a function defined on a concrete finite sets im-plementation can be obtained from a function defined on the abstract finite sets K(A), and correctness properties are in-herited. Hence, HoTT is the ideal setting for data refinement. Beside this, we define bounded quantification which lifts a decidable property on A to one on K(A).},
author = {Frumin, Dan and Geuvers, Herman and Gondelman, L{\'{e}}on and {Van Der Weide}, Niels},
isbn = {9781450355865},
keywords = {acm reference format,and niels van der,coq,dan frumin,finite sets,finite types,herman geuvers,higher inductive types,homotopy type theory,l{\'{e}}on gondelman},
title = {{Finite Sets in Homotopy Type Theory}},
url = {https://goo.gl/Qm8zp9}
}
@article{COCKX2016,
abstract = {This paper gives a reduction-preserving translation from Coquand's dependent pattern matching into a traditional type theory with universes, inductive types and relations and the axiom K. This translation serves as a proof of termination for structurally recur- sive pattern matching programs, provides an implementable compilation technique in the style of functional programming languages, and demon- strates the equivalence with a more easily understood type theory.},
author = {Cockx, Jesper and Devriese, Dominique and Piessens, Frank},
doi = {10.1017/S0956796816000174},
isbn = {978-3-540-35462-8},
issn = {14697653},
journal = {Journal of Functional Programming},
pages = {1--40},
title = {{Eliminating dependent pattern matching without K}},
year = {2016}
}
@article{Escardo2018,
abstract = {In introductions to the subject for a general audience of mathematicians or logicians, the univalence axiom is typically explained by handwaving. This gives rise to several misconceptions, which cannot be properly addressed in the absence of a precise definition. In this short set of notes we give a complete formulation of the univalence axiom from scratch. The underlying idea of these notes is that they should be as concise as possible (and not more). They are not meant to be an Encyclopedia of Univalence.},
archivePrefix = {arXiv},
arxivId = {1803.02294},
author = {Escard{\'{o}}, Mart{\'{i}}n H{\"{o}}tzel},
eprint = {1803.02294},
keywords = {martin-l,of,s identity type,type universe,univalence axiom},
pages = {1--9},
title = {{A self-contained, brief and complete formulation of Voevodsky's Univalence Axiom}},
url = {http://arxiv.org/abs/1803.02294},
year = {2018}
}
@inproceedings{harrison-ab,
abstract = {This paper surveys the field of automated reasoning, giving some historical background and outlining a few of the main current research themes. We particularly emphasize the points of contact and the contrasts with computer algebra. We finish with a discussion of the main applications so far.},
address = {Castle of Hagenberg, Austria},
author = {Harrison, John},
booktitle = {Proceedings of the Second International Conference on Algebraic Biology, AB 2007},
editor = {Anai, Hirokazu and Horimoto, Katsuhisa and Kutsia, Temur},
pages = {334--349},
publisher = {Springer-Verlag},
series = {Lecture Notes in Computer Science},
title = {{A short survey of automated reasoning}},
volume = {4545},
year = {2007}
}
@inproceedings{BauerN02,
author = {Bauer, Gertrud and Nipkow, Tobias},
booktitle = {Theorem Proving in Higher Order Logics, 15th International Conference, TPHOLs 2002, Hampton, VA, USA, August 20-23, 2002, Proceedings},
doi = {10.1007/3-540-45685-6_6},
pages = {67--82},
title = {{The 5 Colour Theorem in Isabelle/Isar}},
url = {https://doi.org/10.1007/3-540-45685-6_6},
year = {2002}
}
@inproceedings{harrison-style,
abstract = {We are concerned with how computer theorem provers should expect users to communicate proofs to them. There are many stylistic choices that still allow the machine to generate a completely formal proof object. The most obvious choice is the amount of guidance required from the user, or from the machine perspective, the degree of automation provided. But another important consideration, which we consider particularly significant, is the bias towards a `procedural' or `declarative' proof style. We will explore this choice in depth, and discuss the strengths and weaknesses of declarative and procedural styles for proofs in pure mathematics and for verification applications. We conclude with a brief summary of our own experiments in trying to combine both approaches.},
address = {Aussois, France},
author = {Harrison, John},
booktitle = {Types for Proofs and Programs: International Workshop {\{}TYPES'96{\}}},
editor = {Gim{\'{e}}nex, Eduardo and Pausin-Mohring, Christine},
pages = {154--172},
publisher = {Springer-Verlag},
series = {Lecture Notes in Computer Science},
title = {{Proof Style}},
volume = {1512},
year = {1996}
}
@inproceedings{harrison-mark10,
author = {Harrison, John},
editor = {Broy, Manfred and Leuxner, Christian and Hoare, Tony},
pages = {103--157},
publisher = {IOS Press},
title = {{Software and Systems Safety - Specification and Verification}},
year = {2011}
}
@article{avigad-fvm,
author = {Avigad, Jeremy and Harrison, John},
journal = {Communications of the ACM},
number = {4},
pages = {66--75},
title = {{Formally Verified Mathematics}},
url = {https://goo.gl/nkhtrq},
volume = {57},
year = {2014}
}
@article{Gonthier2008,
abstract = {Francis Guthrie certainly did it, when he coined his innocent little coloring puzzle in 1852. He man- aged to embarrass successively his mathematician brother, his brother's professor, Augustus de Mor- gan, and all of de Morgan's visitors, who couldn't solve it; the Royal Society, who only realized ten years later that Alfred Kempe's 1879 solution was wrong; and the three following generations of mathematicians who couldn't fix it [19].},
author = {Gonthier, Georges},
doi = {10.1.1.141.714},
journal = {Notices of the AMS},
number = {11},
pages = {1382--1393},
title = {{Formal proof--the four-color theorem}},
volume = {55},
year = {2008}
}
@article{harrison-pnt,
abstract = {We describe the computer formalization of a complex-analytic proof of the Prime Number Theorem (PNT), a classic result from number theory characterizing the asymptotic density of the primes. The formalization, conducted using the HOL Light theorem prover, proceeds from the most basic axioms for mathematics yet builds from that foundation to develop the necessary analytic machinery including Cauchy's integral formula, so that we are able to formalize a direct, modern and elegant proof instead of the more involved `elementary' Erd{\"{o}}s-Selberg argument. As well as setting the work in context and describing the highlights of the formalization, we analyze the relationship between the formal proof and its informal counterpart and so attempt to derive some general lessons about the formalization of mathematics.},
author = {Harrison, John},
journal = {Journal of Automated Reasoning},
pages = {243--261},
title = {{Formalizing an analytic proof of the {\{}P{\}}rime {\{}N{\}}umber {\{}T{\}}heorem (Dedicated to {\{}M{\}}ike {\{}G{\}}ordon on the occasion of his 60th birthday)}},
volume = {43},
year = {2009}
}
@article{hales-kepler,
author = {Hales, Thomas C and Adams, Mark and Bauer, Gertrud and Dang, Dat Tat and Harrison, John and Hoang, Truong Le and Kaliszyk, Cezary and Magron, Victor and McLaughlin, Sean and Nguyen, Thang Tat and Nguyen, Truong Quang and Nipkow, Tobias and Obua, Steven and Pleso, Joseph and Rute, Jason and Solovyev, Alexey and Ta, An Hoai Thi and Tran, Trung Nam and Trieu, Diep Thi and Urban, Josef and Vu, Ky Khac and Zumkeller, Roland},
journal = {arXiv},
title = {{A formal proof of the Kepler conjecture}}
}
@inproceedings{harrison-itp,
author = {Harrison, John and Urban, Josef and Wiedijk, Freek},
title = {{History of Interactive Theorem Proving}}
}
@proceedings{DBLP:conf/tphol/2002,
doi = {10.1007/3-540-45685-6},
editor = {Carre{\~{n}}o, Victor and Mu{\~{n}}oz, C{\'{e}}sar A and Tahar, Sofi{\`{e}}ne},
isbn = {3-540-44039-9},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Theorem Proving in Higher Order Logics, 15th International Conference, TPHOLs 2002, Hampton, VA, USA, August 20-23, 2002, Proceedings}},
url = {https://doi.org/10.1007/3-540-45685-6},
volume = {2410},
year = {2002}
}
@article{Bauer2017,
abstract = {On the odd day, a mathematician might wonder what construc-tive mathematics is all about. They may have heard arguments in favor of constructivism but are not at all convinced by them, and in any case they may care little about philosophy. A typical introductory text about construc-tivism spends a great deal of time explaining the principles and contains only trivial mathematics, while advanced constructive texts are impenetrable, like all unfamiliar mathematics. How then can a mathematician find out what constructive mathematics feels like? What new and relevant ideas does con-structive mathematics have to offer, if any? I shall attempt to answer these questions. From a psychological point of view, learning constructive mathematics is ago-nizing, for it requires one to first unlearn certain deeply ingrained intuitions and habits acquired during classical mathematical training. In her book On Death and Dying [17] psychologist Elisabeth K{\"{u}}bler-Ross identified five stages through which people reach acceptance of life's traumatizing events: denial, anger, bargaining, depression, and acceptance. We shall follow her path.},
author = {Bauer, Andrej},
doi = {10.1090/bull/1556},
issn = {02730979},
journal = {Bulletin of the American Mathematical Society},
number = {3},
pages = {481--498},
title = {{Five stages of accepting constructive mathematics}},
volume = {54},
year = {2017}
}
@article{Grayson2017,
abstract = {We offer an introduction for mathematicians to the univalent foundations of Vladimir Voevodsky, aiming to explain how he chose to encode mathematics in type theory and how the encoding reveals a potentially viable foundation for all of modern mathematics that can serve as an alternative to set theory.},
archivePrefix = {arXiv},
arxivId = {1711.01477},
author = {Grayson, Daniel R.},
eprint = {1711.01477},
title = {{An Introduction to Univalent Foundations for Mathematicians}},
url = {http://arxiv.org/abs/1711.01477},
year = {2017}
}
@phdthesis{Bergeron1998,
abstract = {The combinatorial theory of species, introduced by Joyal in 1980, provides a unified understanding of the use of generating functions for both labeled and unlabeled structures as well as a tool for the specification and analysis of these structures. This key reference presents the basic elements of the theory and gives a unified account of its developments and applications. The authors offer a modern introduction to the use of various generating functions, with applications to graphical enumeration, Polya Theory and analysis of data structures in computer science, and to other areas such as special functions, functional equations, asymptotic analysis, and differential equations.},
author = {Yorgey, Brent Abraham},
isbn = {0521573238},
pages = {206},
title = {{Combinatorial Species And Labelled Structures}},
url = {https://books.google.com/books?id=83odtWY4eogC&pgis=1},
year = {2014}
}
@book{Salomaa2013,
abstract = {This book presents sequential decision theory from a novel algorithmic information theory perspective. While the former is suited for active agents in known environments, the latter is suited for passive prediction in unknown environments. The book introduces these two well-known but very different ideas and removes the limitations by unifying them to one parameter-free theory of an optimal reinforcement learning agent embedded in an arbitrary unknown environment. Most if not all AI problems can easily be formulated within this theory, which reduces the conceptual problems to pure computational ones. Considered problem classes include sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations to other approaches to AI. One intention of this book is to excite a broader AI audience about abstract algorithmic information theory concepts, and conversely to inform theorists about exciting applications to AI.},
author = {Salomaa, Arto},
doi = {10.1007/978-3-642-16533-7},
isbn = {9783540299523},
pages = {1--288},
title = {{Public-Key Cryptography (Texts in Theoretical Computer Science. An EATCS Series)}},
year = {2013}
}
@article{Vieira1984,
abstract = {Presents readers' comments on articles appearing in the "Professional Safety Journal." Appeal for a collaboration between the American Society of Safety Engineers and international organizations; Interrelations between risk-taking behavior and accidents.},
author = {Allen, Christopher and Moronuki, Julie},
doi = {10.1080/07366988409450331},
journal = {Edpacs},
number = {7},
pages = {15},
title = {{Haskell programming from principles}},
volume = {11},
year = {1984}
}
@book{HallCRC2016,
abstract = {The fusion between graph theory and combinatorial optimization has led to theoretically profound and practically useful algorithms, yet there is no book that currently covers both areas together. Handbook of Graph Theory, Combinatorial Optimization, and Algorithms is the first to present a unified, comprehensive treatment of both graph theory and combinatorial optimization. Divided into 11 cohesive sections, the handbook's 44 chapters focus on graph theory, combinatorial optimization, and algorithmic issues. The book provides readers with the algorithmic and theoretical foundations to: Understand phenomena as shaped by their graph structures Develop needed algorithmic and optimization tools for the study of graph structures Design and plan graph structures that lead to certain desirable behavior With contributions from more than 40 worldwide experts, this handbook equips readers with the necessary techniques and tools to solve problems in a variety of applications. Readers gain exposure to the theoretical and algorithmic foundations of a wide range of topics in graph theory and combinatorial optimization, enabling them to identify (and hence solve) problems encountered in diverse disciplines, such as electrical, communication, computer, social, transportation, biological, and other networks.},
author = {Hall/CRC and Chapman},
isbn = {9781420011074},
pages = {1226},
title = {{Handbook of Graph Theory , Combinatorial Optimization , and Algorithms Series Editor : Sartaj Sahni}},
year = {2016}
}
@phdthesis{Bauer,
abstract = {In 1998, Thomas Hales published a proof of the Kepler Conjecture, which states that the cubic close packing is the densest possible packing of equally- sized spheres. The proof is by exhaustion on a set of 3050 plane graphs satisfying certain properties, called tame plane graphs. The enumeration of this set has been generated by a computer program, hence the completeness of this enumeration is essential for the proof. In this thesis, we formalize a theory of plane graphs defined as an inductive set in the theorem prover Isabelle/HOL: a plane graph is constructed starting with one face and repeatedly adding new faces. Based on this theory, we formalize one part of the proof of the Kepler Conjecture, the completeness of the enumeration of tame plane graphs},
author = {Bauer, Gertrud Josefine},
title = {{Formalizing Plane Graph Theory – Towards a Formalized Proof of the Kepler Conjecture}},
url = {https://goo.gl/jxNXcV},
year = {2005}
}
@book{Rahman2017,
author = {Rahman, Md. Saidur},
doi = {10.1007/978-3-319-49475-3},
isbn = {978-3-319-49474-6},
issn = {15715078},
pmid = {1000172844},
title = {{Basic Graph Theory}},
url = {http://link.springer.com/10.1007/978-3-319-49475-3},
year = {2017}
}
@book{Diestel2017,
abstract = {Almost two decades have passed since the appearance of those graph the- ory texts that still set the agenda for most introductory courses taught today. The canon created by those books has helped to identify some main fields of study and research, and will doubtless continue to influence the development of the discipline for some time to come.},
author = {Diestel, Reinhard},
doi = {10.1007/978-3-662-53622-3},
isbn = {978-3-662-53621-6},
title = {{Graph Theory}},
url = {http://link.springer.com/10.1007/978-3-662-53622-3},
volume = {173},
year = {2017}
}
@article{Lammich2017,
author = {Lammich, Peter and Sefidgar, S. Reza},
doi = {10.1007/s10817-017-9442-4},
issn = {0168-7433},
journal = {Journal of Automated Reasoning},
keywords = {Maximum flow problem,Edmonds–Karp algorithm,Push–r,edmonds,formal verification,hol,isabelle,karp algorithm,maximum flow problem,push,relabel algorithm,stepwise refinement},
publisher = {Springer Netherlands},
title = {{Formalizing Network Flow Algorithms: A Refinement Approach in Isabelle/HOL}},
url = {http://link.springer.com/10.1007/s10817-017-9442-4},
year = {2017}
}
@article{Dufourd2009,
author = {Dufourd, Jean Fran{\c{c}}ois},
doi = {10.1007/s10817-009-9117-x},
issn = {01687433},
journal = {Journal of Automated Reasoning},
keywords = {Combinatorial hypermaps,Computational topology,Computer-aided proofs,Coq system,Discrete Jordan Curve Theorem,Formal specifications,Planar subdivisions},
number = {1},
pages = {19--51},
title = {{An intuitionistic proof of a discrete form of the Jordan curve theorem formalized in Coq with combinatorial hypermaps}},
volume = {43},
year = {2009}
}
@article{Noschinski2015,
author = {Noschinski, Lars},
title = {{Formalizing Graph Theory and Planarity Certificates}},
year = {2015}
}
@inproceedings{yamamoto,
abstract = {Among many fields of mathematics and computer science, discrete mathematics is one of the most difficult fields to formalize because we prove theorems using intuitive inferences that have not been rigorously formalized yet. This paper focuses on graph theory from discrete mathematics and formalizes planar graphs. Although planar graphs are usually defined by embeddings into the two-dimensional real space, this definition can hardly be used for actually developing a formal theory of planar graphs. In this paper, we take another approach; we inductively define planar graphs and prove their properties based on the inductive definition. Before the definition of planar graphs, the theory of cycles is also introduced and used as a foundation of planar graphs. As an application of the theory of planar graphs, Euler's formula is proved.},
address = {Berlin, Heidelberg},
author = {Yamamoto, Mitsuharu and Nishizaki, Shin-ya and Hagiya, Masami and Toda, Yozo},
booktitle = {Higher Order Logic Theorem Proving and Its Applications},
isbn = {978-3-540-44784-9},
pages = {369--384},
publisher = {Springer Berlin Heidelberg},
title = {{Formalization of planar graphs}},
url = {https://goo.gl/FwS1Kv},
year = {1995}
}
@book{hottbook,
address = {Institute for Advanced Study},
author = {{Univalent Foundations Program}, The},
keywords = {hott},
mendeley-tags = {hott},
title = {{Homotopy Type Theory: Univalent Foundations of Mathematics}},
url = {http://saunders.phil.cmu.edu/book/hott-online.pdf},
year = {2013}
}
@book{SIAM2013,
author = {SIAM},
isbn = {0644071230},
pages = {122},
title = {{SIAM Style Manual}},
year = {2013}
}
@book{M,
author = {M, Nicholas J.},
title = {{Handbook of writing for the mathematical sciences.pdf}}
}
@article{Turaev,
author = {Turaev, Vladimir and Virelizier, Alexis},
doi = {10.1007/978-3-319-49834-8},
isbn = {9783319498331},
title = {{Monoidal Categories and Topological Field Theory}}
}
@article{Ingene1993,
author = {Ingene, Charles A and Editorials, Guest and Editorial, Guest and Commentaries, Guest and Commentary, Guest},
number = {4},
pages = {467--468},
title = {{Manuscript Guidelines}},
volume = {69},
year = {1993}
}
@book{Tucker2004,
author = {Tucker, Allen B.},
isbn = {158488360X},
number = {March},
title = {{Computer Science Handbook}},
year = {2004}
}
@article{Gallian2000,
author = {Gallian, Joseph A and Higgins, Aparna W},
pages = {289--295},
title = {{Helping Students Present Their Research}},
year = {2000}
}
@book{Mehta2005,
author = {Mehta, Dinesh P. and Sahni, Sartaj},
booktitle = {New York},
isbn = {1584884355},
title = {{Handbook of DATA STRUCTURES and APPLICATIONS}},
year = {2005}
}
@inproceedings{Klein2009,
address = {New York, New York, USA},
author = {Klein, Gerwin and Norrish, Michael and Sewell, Thomas and Tuch, Harvey and Winwood, Simon and Elphinstone, Kevin and Heiser, Gernot and Andronick, June and Cock, David and Derrin, Philip and Elkaduwe, Dhammika and Engelhardt, Kai and Kolanski, Rafal},
booktitle = {Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles - SOSP '09},
doi = {10.1145/1629575.1629596},
isbn = {9781605587523},
keywords = {isabelle/hol,l4,microkernel,sel4},
pages = {207},
publisher = {ACM Press},
title = {{seL4}},
url = {http://portal.acm.org/citation.cfm?doid=1629575.1629596},
year = {2009}
}
@phdthesis{Prieto-Cubides2017a,
abstract = {We describe a syntactical proof-reconstruction approach to verify proofs gener- ated by Metis prover to problems in classical propositional logic. To verify these Metis proofs, we developed a tool able to translate such derivations to Agda proof-terms. We formalise in type theory each inference rule found in Metis derivations. This allowed us to type-check with Agda, Metis proofs step-by-step. We leave it for future work, the opportunity to extend this work to support other provers, proof-assistants, and formalisms.},
author = {Prieto-Cubides, Jonathan},
keywords = {agda,automatic theorem prover,proof-assistant,proof-reconstruction,theorem-proving,type theory},
mendeley-tags = {agda,proof-reconstruction,theorem-proving},
school = {Universidad EAFIT},
title = {{Reconstructing Propositional Proofs in Type Theory}},
url = {https://goo.gl/5whnJd},
year = {2017}
}
@article{Clark2016,
author = {Clark, Patrick G and Gao, Cheng and Grzymala-busse, Jerzy W},
doi = {10.1007/978-3-319-47160-0},
isbn = {978-3-319-47159-4},
keywords = {attribute-concept values,conditions,do not care,incomplete data,mlem2 rule induction algo-,probabilistic approximations},
pages = {65--74},
title = {{Rule Set Complexity for Incomplete Data Sets with Many Attribute-Concept Values and Do Not Care Conditions}},
url = {http://link.springer.com/10.1007/978-3-319-47160-0},
volume = {9920},
year = {2016}
}
@article{Zeng2015a,
abstract = {With the development of the Internet of Things (IoT), more and more hybrid data is being collected by information systems, which are known as Hybrid Information Systems (HIS). Based on a new hybrid distance, novel Gaussian kernel Fuzzy Rough Sets (FRS) for HIS were constructed in our previous study. In real-world applications, with the deepening of cognition and improvements in technology, attribute values in an information system often evolve over time; in particular, there are three cases: when missing values are imputed, error values are corrected, and the values are coarsened or refined. This has posed challenges to developing efficient data analysis algorithms. In this paper, the changing mechanisms of the attribute values and fuzzy equivalence relations in FRS are analyzed. FRS approaches for incrementally updating approximations in HIS are presented. Moreover, two corresponding incremental algorithms are developed. Finally, extensive experiments on eight data sets from the University of California, Irvine (UCI) and an artificial data set show that incremental approaches can effectively improve the performance of updating approximations and not only significantly shorten the computational time, but also increase approximation classification accuracies.},
author = {Zeng, Anping and Li, Tianrui and Luo, Chuan and Hu, Jie},
doi = {10.1109/ICMLC.2015.7340915},
isbn = {9781467372213},
issn = {21601348},
journal = {Proceedings - International Conference on Machine Learning and Cybernetics},
keywords = {Fuzzy rough sets,Hybrid information systems,Incremental learning},
pages = {157--162},
publisher = {Elsevier Inc.},
title = {{Dynamical updating fuzzy rough approximations for hybrid data under the variation of attribute values}},
volume = {1},
year = {2017}
}
@article{Luo2015,
abstract = {Rough set theory is advocated as a framework for conceptualizing and analyzing various types of data, which is a powerful tool for discovering patterns in a given data set through a pair of concepts, namely, upper and lower approximations. Strategic behaviors need to be reinforced continuously under the dynamic decision environment where data in the decision process can change over time. Incremental learning is an effective technique to deal with dynamic learning tasks since it can make full use of previous knowledge. Set-valued data, in which a set of values are associated with an individual, is common in real-world data sets. Motivated by the needs of knowledge updating due to the dynamic variation of criteria values in the set-valued decision system, in this paper, we present the updating properties for dynamic maintenance of approximations when the criteria values in the set-valued decision system evolve with time. Then, two incremental algorithms for computing rough approximations are proposed corresponding to the addition and removal of criteria values, respectively. Experimental results show our incremental algorithms work successfully on datasets from UCI as well as artificial datasets, and achieve better performance than the traditional non-incremental method.},
author = {Luo, Chuan and Li, Tianrui and Chen, Hongmei and Lu, Lixia},
doi = {10.1016/j.ins.2014.12.029},
issn = {00200255},
journal = {Information Sciences},
keywords = {Incremental learning,Knowledge discovery,Rough set,Set-valued decision systems},
pages = {221--242},
publisher = {Elsevier Inc.},
title = {{Fast algorithms for computing rough approximations in set-valued decision systems while updating criteria values}},
url = {http://dx.doi.org/10.1016/j.ins.2014.12.029},
volume = {299},
year = {2015}
}
@article{Zhang2016,
abstract = {Interval-valued Information System (IvIS) is a generalized model of single-valued information system, in which the attribute values of objects are all interval values instead of single values. The attribute set in IvIS is not static but rather dynamically changing over time with the collection of new information, which results in the continuous updating of rough approximations for rough set-based data analysis. In this paper, on the basis of the similarity-based rough set model in IvIS, we develop incremental approaches for updating rough approximations in IvIS under attribute generalization, which refers to the dynamic changing of attributes. Firstly, increment relationships between the original rough approximations and the updated ones when adding or deleting an attribute set are analyzed, respectively. And the incremental mechanisms for updating rough approximations in IvIS are introduced, which carry out the computation using the previous results from the original data set along with new results. Then, the corresponding incremental algorithms are designed based on the proposed mechanisms. Finally, comparative experiments on data sets from UCI as well as artificial data sets are conducted, respectively. Experimental results show that the proposed incremental algorithms can effectively reduce the running time for the computation of rough approximations in comparison with the static algorithm.},
author = {Zhang, Yingying and Li, Tianrui and Luo, Chuan and Zhang, Junbo and Chen, Hongmei},
doi = {10.1016/j.ins.2016.09.018},
isbn = {9783319257532},
issn = {00200255},
journal = {Information Sciences},
keywords = {Approximations,Incremental updating,Interval-valued information system,Rough set,Similarity degree},
pages = {1339--1351},
publisher = {Elsevier Inc.},
title = {{Incremental updating of rough approximations in interval-valued information systems under attribute generalization}},
url = {http://dx.doi.org/10.1016/j.ins.2016.09.018},
volume = {373},
year = {2016}
}
@article{Zhang2015,
abstract = {As the volume of data grows at an unprecedented rate, large-scale data mining and knowledge discovery present a tremendous challenge. Rough set theory, which has been used successfully in solving problems in pattern recognition, machine learning, and data mining, centers around the idea that a set of distinct objects may be approximated via a lower and upper bound. In order to obtain the benefits that rough sets can provide for data mining and related tasks, efficient computation of these approximations is vital. The recently introduced cloud computing model, MapReduce, has gained a lot of attention from the scientific community for its applicability to large-scale data analysis. In previous research, we proposed a MapReduce-based method for computing approximations in parallel, which can efficiently process complete data but fails in the case of missing (incomplete) data. To address this shortcoming, three different parallel matrix-based methods are introduced to process large-scale, incomplete data. All of them are built on MapReduce and implemented on Twister that is a lightweight MapReduce runtime system. The proposed parallel methods are then experimentally shown to be efficient for processing large-scale data.},
author = {Zhang, Junbo and Wong, Jian Syuan and Pan, Yi and Li, Tianrui},
doi = {10.1109/TKDE.2014.2330821},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {MapReduce,Rough sets,data mining,incomplete information systems,matrix},
number = {2},
pages = {326--339},
title = {{A parallel matrix-based method for computing approximations in incomplete information systems}},
volume = {27},
year = {2015}
}
@incollection{Book,
author = {Book, The},
title = {{TLDR Pages}}
}
@article{Gradel2009,
author = {Gr{\"{a}}del, Erich},
title = {{Complexity Theory}},
year = {2009}
}
@incollection{Sicard-Ramirez2016,
address = {Medell{\'{i}}n, Colombia},
author = {Sicard-Ram{\'{i}}rez, Andr{\'{e}}s and Ospina-Giraldo, Juan-Fernando},
publisher = {Universidad EAFIT},
title = {{First-Order Proof Reconstruction (Research Proposal)}},
year = {2016}
}
@article{Paulson1986,
abstract = {Martin-L{\"{o}}f's Intuitionistic Theory of Types is becoming popular for formal reasoning about computer programs. To handle recursion schemes other than primitive recursion, a theory of well-founded relations is presented. Using primitive recursion over higher types, induction and recursion are formally derived for a large class of well-founded relations. Included are {\textless} on natural numbers, and relations formed by inverse images, addition, multiplication, and exponentiation of other relations. The constructions are given in full detail to allow their use in theorem provers for Type Theory, such as PRL. The theory is compared with work in the field of ordinal recursion over higher types. {\textcopyright} 1986, Academic Press Inc. (London) Ltd.. All rights reserved.},
author = {Paulson, Lawrence C.},
doi = {10.1016/S0747-7171(86)80002-5},
issn = {07477171},
journal = {Journal of Symbolic Computation},
number = {4},
pages = {325--355},
title = {{Constructing recursion operators in intuitionistic type theory}},
volume = {2},
year = {1986}
}
@misc{Jordan2003,
abstract = {This book is designed for students embarking on further studies through the medium of English. This course is suitable for students at Cambrdige First Certificate level and above. The third edition has been revised to take account of feedback from users and recent developments in the teaching of writing.},
author = {Jordan, R R},
isbn = {0582400198},
keywords = {engels,english,grammar,grammatica,languages,schrijfvaardigheden,studieboeken,talen,textbooks,writing skills},
pmid = {10682171},
title = {{Academic writing course : study skills in English}},
year = {2003}
}
@book{Steinberg2008,
abstract = {Perfect Phrases to stand out on the TOEFL-for the more than 800,000 people who take the test To be accepted into most North American undergraduate and graduate programs, international students must take and pass the Test of English as a Foreign Language. Perfect Phrases for the TOEFL Speaking and Writing Sections gives you all the phrases and most commonly used words you need to excel on both the writing and speaking sections of the test. Presented in the easy-to-understand Perfect Phrases format, these phrases allow you to effectively communicate and express yourself in standard American English, and to score your very best on the test.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Steinberg, Roberta},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
isbn = {0071592474},
issn = {0717-6163},
pages = {224},
pmid = {15003161},
title = {{Perfect Phrases for the TOEFL Speaking and Writing Sections}},
url = {https://books.google.com/books?id=S8woospF2SYC&pgis=1},
year = {2008}
}
@article{Beringer2015,
abstract = {We have proved, with machine-checked proofs in Coq, that an OpenSSL implementation of HMAC with SHA-256 correctly implements its FIPS functional specifi-cation and that its functional specification guarantees the expected cryptographic properties. This is the first machine-checked cryptographic proof that combines a source-program implementation proof, a compiler-correctness proof, and a cryptographic-security proof, with no gaps at the specification interfaces. The verification was done using three systems within the Coq proof assistant: the Foundational Cryptogra-phy Framework, to verify crypto properties of functional specs; the Verified Software Toolchain, to verify C pro-grams w.r.t. functional specs; and CompCert, for verified compilation of C to assembly language.},
author = {Beringer, Lennart and Petcher, Adam and Ye, Katherine Q and Appel, Andrew W},
doi = {10.1145/3133956.3133974},
isbn = {9781931971232},
journal = {Usenix Sec},
title = {{Verified correctness and security of OpenSSL HMAC}},
year = {2015}
}
@article{Milewski2014,
author = {Milewski, Bartosz},
title = {{Category Theory for Programmers}},
url = {https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/},
year = {2014}
}
@article{Narayan2008,
abstract = {Call graphs depict the static, caller-callee relation between "functions " in a program. With most source/target languages supporting functions as the primitive unit of composition, call graphs naturally form the fundamental control flow representation available to understand/develop software. They are also the substrate on which various inter- procedural analyses are performed and are integral part of program comprehension/testing. Given their universality and usefulness, it is imperative to ask if call graphs exhibit any intrinsic graph theoretic features - across versions, program domains and source languages. This work is an attempt to answer these questions: we present and investigate a set of meaningful graph measures that help us understand call graphs better; we establish how these measures correlate, if any, across different languages and program domains; we also assess the overall, language independent software quality by suitably interpreting these measures.},
archivePrefix = {arXiv},
arxivId = {0803.4025},
author = {Narayan, Ganesh and Gopinath, K. and Sridhar, V.},
doi = {10.1109/TASE.2008.40},
eprint = {0803.4025},
isbn = {9780769532493},
issn = {0018-9219},
journal = {Proceedings - 2nd IFIP/IEEE International Symposium on Theoretical Aspects of Software Engineering, TASE 2008},
pages = {73--80},
pmid = {3524258},
title = {{Structure and interpretation of computer programs}},
year = {2008}
}
@inproceedings{Sutcliffe-Schulz-Claessen-VanGelder-2006,
author = {Sutcliffe, Geoff and Schulz, Stephan and Claessen, Koen and {Van Gelder}, Allen},
booktitle = {International Joint Conference on Automated Reasoning (IJCAR 2006)},
editor = {Furbach, Ulrich and Shankar, Natarajan},
pages = {67--81},
publisher = {Springer},
title = {{Using the TPTP Language for Writing Derivations and Finite Interpretations}},
volume = {4130},
year = {2006}
}
@book{Bertot2004,
abstract = {Coq is an interactive proof assistant for the development of mathematical theories and formally certified software. It is based on a theory called the calculus of inductive constructions, a variant of type theory. This book provides a pragmatic introduction to the development of proofs and certified programs using Coq. With its large collection of examples and exercises it is an invaluable tool for researchers, students, and engineers interested in formal methods and the development of zero-fault software.},
author = {Bertot, Yves and Cast{\'{e}}ran, Pierre},
booktitle = {Springer},
doi = {10.1007/978-3-662-07964-5},
isbn = {3540208542},
title = {{Interactive Theorem Proving and Program Development: Coq'Art: The Calculus of Inductive Constructions}},
url = {http://books.google.nl/books/about/Interactive_Theorem_Proving_and_Program.html?id=m5w5PRj5Nj4C&pgis=1},
year = {2004}
}
@misc{Coquand1992,
abstract = {Pattern Matching with Dependent Types Thierry Coquand Chalmers University Preliminary version, June 1992 Introduction This note deals with notation in type theory. The de nition of a function by pattern matching is by now common, and quite important in practice, in ...},
author = {Coquand, T.},
booktitle = {Informal proceedings of Logical Frameworks},
doi = {10.1.1.37.9541},
number = {June},
pages = {1--14},
title = {{Pattern matching with dependent types}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.9541&rep=rep1&type=pdf#page=69},
volume = {1992},
year = {1992}
}
@article{Bezem2002,
author = {Bezem, Marc and Hendriks, Dimitri and {De Nivelle}, Hans},
doi = {10.1023/A:1021939521172},
isbn = {3540676643},
issn = {01687433},
journal = {Journal of Automated Reasoning},
keywords = {Proof construction,Resolution theorem proving,Type theory},
number = {3-4},
pages = {253--275},
title = {{Automated proof construction in type theory using resolution}},
volume = {29},
year = {2002}
}
@article{Abel2002,
abstract = {We introduce a language based upon lambda calculus with products, coproducts and strictly positive inductive types that allows the definition of recursive terms.  We present the implementation (foetus) of a syntactical check that ensures that all such terms are structurally recursive, i.e. recursive calls appear only with arguments structurally smaller than the input parameters of terms considered.  To ensure the correctness of the termination checker, we show that all structurally recursive terms are normalizing with respect to a given operational semantics.  To this end, we define a semantics on all types and a structural ordering on the values in this semantics and prove that all values are accessible with regard to this ordering.  Finally, we point out how to do this proof predicatively using set based operators.},
author = {Abel, Andreas and Altenkirch, Thorsten},
doi = {10.1017/S0956796801004191},
issn = {0956-7968},
journal = {Journal of Functional Programming},
number = {01},
pages = {1--41},
title = {{A Predicative Analysis of Structural Recursion}},
url = {http://www.journals.cambridge.org/abstract_S0956796801004191},
volume = {12},
year = {2002}
}
@book{VanDalen1994,
address = {Berlin, Heidelberg},
annote = {The process of formalization of propositional logic consists of two stages:
(1) present a formal language, (2) specify a procedure for obtaining valid or
true propositions.},
author = {van Dalen, Dirk},
doi = {10.1007/978-3-662-02962-6},
isbn = {978-3-540-57839-0},
publisher = {Springer Berlin Heidelberg},
series = {Universitext},
title = {{Logic and Structure}},
url = {http://link.springer.com/10.1007/978-3-662-02962-6},
year = {1994}
}
@article{Stefanowski2001,
abstract = {The rough set theory, based on the original definition of the indiscernibility relation, is not useful for analysing incomplete information tables where some values of attributes are unknown. In this paper we distinguish two different semantics for incomplete information: the "missing value" semantics and the "absent value" semantics. The already known approaches, e.g. based on the tolerance relations, deal with the missing value case. We introduce two generalisations of the rough sets theory to handle these situations. The first generalisation introduces the use of a non symmetric similarity relation in order to formalise the idea of absent value semantics. The second proposal is based on the use of valued tolerance relations. A logical analysis and the computational experiments show that for the valued tolerance approach it is possible to obtain more informative approximations and decision rules than using the approach based on the simple tolerance relation.},
author = {Stefanowski, J. and Tsouki{\`{a}}s, A.},
doi = {10.1111/0824-7935.00162},
issn = {08247935},
journal = {Computational Intelligence},
keywords = {Decision rules,Fuzzy sets,Incomplete information,Rough sets,Similarity relation,Valued tolerance relation},
number = {3},
pages = {545--566},
title = {{Incomplete information tables and rough classification}},
volume = {17},
year = {2001}
}
@article{Agudelo-Agudelo2017,
author = {Agudelo-Agudelo, Juan C.},
doi = {10.1007/s11787-017-0168-1},
issn = {1661-8297},
journal = {Logica Universalis},
number = {2},
pages = {205--224},
title = {{Translating Non-classical Logics into Classical Logic by Using Hidden Variables}},
url = {http://link.springer.com/10.1007/s11787-017-0168-1},
volume = {11},
year = {2017}
}
@article{Bove2002,
abstract = {General recursive algorithms are such that the recursive calls are performed on arguments satisfying no condition that guarantees termination. Hence, there is no direct way of formalising them in type theory. The standard way of handling general recursion in type theory uses a well-founded recursion principle. Unfortunately, this way of formalising general recursive algorithms often produces unnecessarily long and complicated codes. On the other hand, functional programming languages like Haskell impose no restrictions on recursive programs, and then writing general recursive algorithms is straightforward. In addition, functional programs are usually short and self-explanatory. However, the existing frameworks for reasoning about the correctness of Haskell-like programs are weaker than the framework provided by type theory. The goal of this work is to present a method that combines the advantages of both programming styles when writing simple general recursive algorithms....},
author = {Bove, Ana},
doi = {10.1017/S0960129505004822},
issn = {0346718X},
journal = {Doktorsavhandlingar vid Chalmers Tekniska Hogskola},
number = {1889},
pages = {39--58},
title = {{General recursion in type theory}},
year = {2002}
}
@incollection{Bove2005,
author = {Bove, Ana and Capretta, Venanzio},
doi = {10.1007/11417170_10},
pages = {116--130},
publisher = {Springer, Berlin, Heidelberg},
title = {{Recursive Functions with Higher Order Domains}},
url = {http://link.springer.com/10.1007/11417170_10},
year = {2005}
}
@article{Luo2014,
author = {Luo, Chuan and Li, Tianrui},
doi = {10.1007/978-3-319-08644-6_13},
isbn = {9783319086439},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {incomplete decision system,incremental updating,three-way decisions},
pages = {128--135},
title = {{Incremental three-way decisions with incomplete information}},
volume = {8536 LNAI},
year = {2014}
}
@article{Liu2015,
abstract = {As a natural extension of three-way decisions with incomplete information, this paper provides a novel three-way decision model based on incomplete information system. First, we define a new relation to describe the similarity degree of incomplete information. Then, in view of the missing values presented in incomplete information system, we utilize interval number to acquire the loss function. A hybrid information table which consist both of the incomplete information and loss function, is used to deal with the new three-way decision model. The key steps and algorithm for constructing the integrated three-way decision model are also carefully investigated. An empirical study of medical diagnosis validates the reasonability and effectiveness of our proposed model.},
author = {Liu, Dun and Liang, Decui and Wang, Changchun},
doi = {10.1016/j.knosys.2015.07.036},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Decision-theoretic rough sets,Hybrid information system,Incomplete information system,Loss function,Three-way decisions},
number = {July},
pages = {32--45},
publisher = {Elsevier B.V.},
title = {{A novel three-way decision model based on incomplete information system}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705115003007},
volume = {91},
year = {2015}
}
@book{Buurlage,
author = {Buurlage, Jan-willem},
title = {{Categories and Haskell}},
year = {2018}
}
@article{Sutcliffe1996,
author = {Sutcliffe, Geoff},
keywords = {automatic theorem proving,clauses,computing review categories,resolution},
number = {18},
pages = {57--68},
title = {{The Practice of Clausification in Automatic Theorem Proving}},
year = {1996}
}
@phdthesis{Sicard2015,
abstract = {We propose a new approach to computer-assisted verification of lazy functional programs where functions can be defined by general recursion. We work in first-order theories of functional programs which are obtained by translating Dybjer's programming logic (Dybjer, P. [1985]. Program Veri- fication in a Logical Theory of Constructions. In: Functional Programming Languages and Computer Architecture. Ed. by Jouannaud, J.-P. Vol. 201. Lecture Notes in Computer Science. Springer, pp. 334–349) into a first-order theory, and by extending this programming logic with new (co-)inductive predicates. Rather than building a special purpose system, we formalise our theories in Agda, a proof assistant for dependent type theory which can be used as a generic theorem prover. Agda provides support for interact- ive reasoning by representing first-order theories using the propositions-as- types principle. Further support is provided by off-the-shelf automatic the- orem provers for first-order-logic called by a Haskell program that translates our Agda representations of first-order formulae into the TPTP language understood by the provers. We show some examples where we combine interactive and automatic reasoning, covering both proofs by induction and co- induction. The examples include functions defined by structural recursion, simple general recursion, nested recursion, higher-order recursion, guarded and unguarded co-recursion. Keywords:},
author = {Sicard-Ram{\'{i}}rez, Andr{\'{e}}s and Bove, Ana and Dybjer, Peter},
keywords = {automatic proofs,correctness,first-order theories,functional program,general recursion,interactive proofs,lazy evaluation,total lan- guages,type theory},
school = {Universidad de la Rep{\{}{\'{u}}{\}}blica},
title = {{Reasoning about Functional Programs by Combining Interactive and Automatic Proofs}},
url = {https://www.colibri.udelar.edu.uy/handle/123456789/4715},
year = {2015}
}
@misc{OnlineATPs,
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/zenodo.398851},
title = {{OnlineATPs: A command-line tool client for the TPTP World.}},
url = {https://doi.org/10.5281/zenodo.398851},
year = {2017}
}
@misc{AgdaMetis,
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/zenodo.398862},
title = {{Metis Prover Reasoning for Propositional Logic in Agda}},
url = {https://doi.org/10.5281/zenodo.398862},
year = {2017}
}
@misc{Athena,
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/zenodo.437196},
title = {{A Translator Tool for Metis Proofs in Haskell}},
url = {https://doi.org/10.5281/zenodo.437196},
year = {2017}
}
@misc{AgdaProp,
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/zenodo.398852},
title = {{A Library for Classical Propositional Logic in Agda}},
url = {https://doi.org/10.5281/zenodo.398852},
year = {2017}
}
@misc{Prieto-Cubides2017,
abstract = {Collection of TPTP problems of propositional logic.},
author = {Prieto-Cubides, Jonathan},
doi = {10.5281/ZENODO.817997},
title = {{A Collection of Propositional Problems in TPTP Format}},
url = {https://zenodo.org/record/817997#.WYdjIbb5iAw},
year = {2017}
}
@inproceedings{Armand2010,
abstract = {Coq has within its logic a programming language that can be used to replace many deduction steps into a single computation, this is the so-called reflection. In this paper, we present two extensions of the evaluation mechanism that preserve its correctness and make it possible to deal with cpu-intensive tasks such as proof checking of SAT traces.},
author = {Armand, Michael and Gr{\'{e}}goire, Benjamin and Spiwack, Arnaud and Th{\'{e}}ry, Laurent},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-14052-5_8},
isbn = {3642140513},
issn = {03029743},
pages = {83--98},
title = {{Extending Coq with imperative features and its application to SAT verification}},
volume = {6172 LNCS},
year = {2010}
}
@inproceedings{Bove2012,
abstract = {We propose a new approach to the computer-assisted verification of functional programs. We work in first order theories of functional programs which are obtained by extending Aczel's first order theory of combinatory formal arithmetic with positive inductive and coinductive predicates. Rather than building a special purpose system we implement our theories in Agda, a proof assistant for dependent type theory which can be used as a generic theorem prover. Agda provides support for interactive reasoning by encoding first order theories using the formulae-as-types principle. Further support is provided by off-the-shelf automatic theorem provers for first order logic which can be called by a program which translates Agda representations of first order formulae into the TPTP language understood by the provers. We show some examples where we combine interactive and automatic reasoning, covering both proof by induction and coinduction.},
author = {Bove, Ana and Dybjer, Peter and Sicard-Ram{\'{i}}rez, Andr{\'{e}}s},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-28729-9_7},
isbn = {9783642287282},
issn = {03029743},
pages = {104--118},
publisher = {Springer},
title = {{Combining Interactive and Automatic Reasoning in First Order Theories of Functional Programs}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-28729-9_7},
volume = {7213 LNCS},
year = {2012}
}
@article{Ge2008,
abstract = {Satisfiability Modulo Theories (SMT) solvers are large and complicated pieces of code. As a result, ensuring their correctness is challenging. In this paper, we discuss a technique for ensuring soundness by producing and checking proofs. We give details of our implementation using CVC3 and HOL Light and provide initial results from our effort to certify the SMT-LIB benchmarks.},
author = {Ge, Yeting and Barrett, Clark},
journal = {Proceedings of the 6{\^{}}{\{}th{\}} International Workshop on Satisfiability Modulo Theories (SMT '08)},
number = {0551645},
pages = {1--11},
title = {{Proof Translation and SMT-LIB Benchmark Certification: A Preliminary Report}},
url = {https://goo.gl/HwUUUK},
year = {2008}
}
@article{Church1940,
abstract = {Simple type theory is formulated for use with the generic theorem prover Isabelle. This requires explicit type inference rules. There are function, product, and subset types, which may be empty. Descriptions (the eta-operator) introduce the Axiom of Choice. Higher-order logic is obtained through reflection between formulae and terms of type bool. Recursive types and functions can be formally constructed. Isabelle proof procedures are described. The logic appears suitable for general mathematics as well as computational problems.},
archivePrefix = {arXiv},
arxivId = {cs/9301107},
author = {Church, Alonzo},
doi = {10.2307/2266170},
eprint = {9301107},
issn = {00224812},
journal = {The Journal of Symbolic Logic},
number = {2},
pages = {56--68},
primaryClass = {cs},
title = {{A Formulation of the Simple Theory of Types}},
url = {http://www.jstor.org/stable/2266170},
volume = {5},
year = {1940}
}
@article{Kanso2012,
author = {Kanso, Karim},
title = {{Agda as a Platform for the Development of Verified Railway Interlocking Systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.310.1502},
year = {2012}
}
@phdthesis{kanso:MResThesis:2013,
abstract = {This project studied whether a digital interlocking which had been pro- grammed with ladder logic (Boolean program) would obey generic safety properties. This was carried out by translating the ladder logic into an alternate representation and applying various techniques to allow specifica- tion of safety properties. Finally, a proof engine was used to formally verify if these properties were fulfilled and if they are not, then human readable documentation would be generated. III},
annote = {{\{}M{\}}aster of {\{}R{\}}esearch thesis, Dept.{\~{}}of Computer Science, Swansea University, Swansea SA2 8PP, UK.
Available from http://www.swan.ac.uk/{\$}{\~{}}{\$}csetzer/articlesFromOthers/index.html},
author = {Kanso, Karim},
school = {Swansea University},
title = {{Formal Verification of Ladder Logic}},
url = {https://goo.gl/1Mj3oa},
year = {2010}
}
@inproceedings{Stump2008,
address = {New York, New York, USA},
author = {Stump, Aaron and Oe, Duckki},
booktitle = {Proceedings of the Joint Workshops of the 6th International Workshop on Satisfiability Modulo Theories and 1st International Workshop on Bit-Precise Reasoning - SMT '08/BPR '08},
doi = {10.1145/1512464.1512470},
isbn = {9781605584409},
keywords = {logical framework,proof formats},
pages = {27},
publisher = {ACM Press},
title = {{Towards an SMT proof format}},
url = {http://portal.acm.org/citation.cfm?doid=1512464.1512470},
year = {2008}
}
@article{Tammet1997,
abstract = {We give a brief overview of the first-order classical logic component in the Gandalf family of resolution-based automated theorem provers for classical and intuitionistic logics. The main strength of the described version is a sophisticated algorithm for nonunit subsumption. {\textcopyright} 1997 Kluwer Academic Publishers.},
author = {Tammet, Tanel},
issn = {01687433},
journal = {Journal of Automated Reasoning},
keywords = {Automated theorem proving,Competition,Gandalf,Resolution,Subsumption},
number = {2},
pages = {199--204},
title = {{Gandalf}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0031108576&partnerID=tZOtx3y1},
volume = {18},
year = {1997}
}
@inproceedings{Brown2012,
abstract = {Satallax is an automatic higher-order theorem prover that generates propositional clauses encoding (ground) tableau rules and uses MiniSat to test for unsatisfiability. We describe the implementation, focusing on flags that control search and examples that illustrate how the search proceeds.},
author = {Brown, Chad E.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-31365-3_11},
isbn = {9783642313646},
issn = {03029743},
keywords = {higher-order logic,higher-order theorem proving,simple type theory},
pages = {111--117},
title = {{Satallax: An automatic higher-order prover}},
volume = {7364 LNAI},
year = {2012}
}
@inproceedings{Benzmuller2008,
abstract = {LEO-II is a standalone, resolution-based higher-order theorem prover designed for effective cooperation with specialist provers for natural fragments of higher-order logic. At present LEO-II can cooperate with the first-order automated theorem provers E, SPASS, and Vampire. The improved performance of LEO-II, especially in comparison to its predecessor LEO, is due to several novel features including the exploitation of term sharing and term indexing techniques, support for primitive equality reasoning, and improved heuristics at the calculus level. LEO-II is implemented in Objective Caml and its problem representation language is the new TPTP THF language.},
author = {Benzm{\"{u}}ller, Christoph and Paulson, Lawrence C. and Theiss, Frank and Fietzke, Arnaud},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-71070-7_14},
isbn = {3540710698},
issn = {03029743},
pages = {162--170},
title = {{LEO-II - A cooperative automatic theorem prover for classical higher-order logic (system description)}},
volume = {5195 LNAI},
year = {2008}
}
@inproceedings{Lonsing2017,
abstract = {We present the latest major release version 6.0 of the quanti-fied Boolean formula (QBF) solver DepQBF, which is based on QCDCL. QCDCL is an extension of the conflict-driven clause learning (CDCL) paradigm implemented in state of the art propositional satisfiability (SAT) solvers. The Q-resolution calculus (QRES) is a QBF proof system which underlies QCDCL. QCDCL solvers can produce QRES proofs of QBFs in prenex conjunctive normal form (PCNF) as a byproduct of the solving process. In contrast to traditional QCDCL based on QRES, DepQBF 6.0 implements a variant of QCDCL which is based on a generalization of QRES. This generalization is due to a set of additional axioms and leaves the original Q-resolution rules unchanged. The generalization of QRES enables QCDCL to potentially produce exponentially shorter proofs than the traditional variant. We present an overview of the features imple-mented in DepQBF and report on experimental results which demonstrate the effectiveness of generalized QRES in QCDCL.},
address = {Gotenburg},
author = {Lonsing, Florian and Egly, Uwe},
booktitle = {International Conference on Automated Deduction},
publisher = {Springer},
title = {{DepQBF 6.0: A Search-Based QBF Solver Beyond Traditional QCDCL}},
url = {https://arxiv.org/pdf/1702.08256.pdf},
year = {2017}
}
@phdthesis{Klieber2014,
author = {Klieber, William},
keywords = {QBF Solvers},
mendeley-tags = {QBF Solvers},
school = {Carnegie Mellon University},
title = {{Formal Verification Using Quantified Boolean Formulas (QBF)}},
url = {https://goo.gl/Mokhmz},
year = {2014}
}
@incollection{Een2004,
author = {E{\'{e}}n, Niklas and S{\"{o}}rensson, Niklas},
doi = {10.1007/978-3-540-24605-3_37},
pages = {502--518},
publisher = {Springer, Berlin, Heidelberg},
title = {{An Extensible SAT-solver}},
url = {http://link.springer.com/10.1007/978-3-540-24605-3_37},
year = {2004}
}
@inproceedings{Moskewicz2001,
abstract = {Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search - especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO.},
author = {Moskewicz, Matthew W and Madigan, Conor F. and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
booktitle = {Proceedings of the 38th Design Automation Conference (DAC 2001)},
doi = {http://doi.acm.org/10.1145/378239.379017},
isbn = {1-58113-297-2},
issn = {0738-100X},
keywords = {Branching Heuristics,CDCL,Chaff,Lazy Data-Structure,SAT,Solver,VSIDS,Watched literals},
pages = {530--535},
title = {{Chaff: engineering an efficient SAT solver}},
year = {2001}
}
@misc{Schmitt2001,
author = {Schmitt, Stephan and Lorigo, Lori and Kreitz, Christoph and Nogin, Aleksey},
doi = {10.1007/3-540-45744-5_34},
pages = {421--426},
publisher = {Springer, Berlin, Heidelberg},
title = {{JProver: Integrating Connection-Based Theorem Proving into Interactive Proof Assistants}},
url = {http://link.springer.com/10.1007/3-540-45744-5_34},
year = {2001}
}
@incollection{Otten2008,
address = {Berlin, Heidelberg},
author = {Otten, Jens},
booktitle = {Automated Reasoning},
doi = {10.1007/978-3-540-71070-7_23},
pages = {283--291},
publisher = {Springer Berlin Heidelberg},
title = {{leanCoP 2.0 and ileanCoP 1.2: High Performance Lean Theorem Proving in Classical and Intuitionistic Logic (System Descriptions)}},
url = {http://link.springer.com/10.1007/978-3-540-71070-7_23},
year = {2008}
}
@incollection{Tammet1996,
author = {Tammet, Tanel},
doi = {10.1007/3-540-61511-3_65},
pages = {2--16},
publisher = {Springer, Berlin, Heidelberg},
title = {{A Resolution Theorem Prover for Intuitionistic Logic}},
url = {http://link.springer.com/10.1007/3-540-61511-3_65},
year = {1996}
}
@techreport{Gomez-Londono2015,
author = {G{\'{o}}mez-Londo{\~{n}}o, Alejandro},
institution = {EAFIT University},
title = {{Proof Reconstruction: Parsing Proofs}},
url = {http://repository.eafit.edu.co/handle/10784/5484},
year = {2015}
}
@article{Ekici2017,
abstract = {This paper describes SMTCoq, a plug-in for the integration of external solvers into the Coq proof assistant. Based on a checker for generic first-order proof certificates fully implemented and proved correct in Coq, SMTCoq offers facilities to check answers from external SAT and SMT solvers and to increase Coq's automation using such solvers, all in a safeway. The current version supports proof certificates produced by the SAT solver ZChaff, for propositional logic, and the SMT solvers veriT and CVC4, for the quantifier-free fragment of the combined theory of fixed-size bit vectors, functional arrays with extensionality, linear integer arithmetic, and uninterpreted function symbols.},
author = {Ekici, B. and Mebsout, A. and Tinelli, C. and Keller, C. and Katz, G.},
journal = {stanford.edu},
title = {{SMTCoq: A plug-in for integrating SMT solvers into Coq}},
url = {https://goo.gl/3JKsyc},
year = {2017}
}
@phdthesis{Keller2013,
author = {Keller, Chantal},
title = {{A Matter of Trust: Skeptical Communication Between Coq and External Provers}},
url = {https://hal.archives-ouvertes.fr/pastel-00838322/},
year = {2013}
}
@inproceedings{Burel,
abstract = {The $\lambda$$\Pi$-calculus modulo is a proof language that has been proposed as a proof standard for (re-)checking and interoperability. Resolution and superposition are proof-search methods that are used in state-of-the-art first-order automated theorem provers. We provide a shallow embedding of resolution and superposition proofs in the $\lambda$$\Pi$-calculus modulo, thus offering a way to check these proofs in a trusted setting, and to combine them with other proofs. We implement this embedding as a backend of the prover iProver Modulo.},
author = {Burel, Guillaume},
booktitle = {Third International Workshop on Proof Exchange for Theorem Proving,},
keywords = {Interoperability,automatic theorem provers,proof checking,rewriting},
mendeley-tags = {Interoperability,automatic theorem provers,proof checking,rewriting},
pages = {1--15},
title = {{A Shallow Embedding of Resolution and Superposition Proofs into the $\lambda$ $\Pi$ -Calculus}},
year = {2013}
}
@article{Mossakowski2005,
author = {Mossakowski, T. and Goguen, J. and Diaconescu, R. and Tarlecki, A.},
journal = {Logica universalis},
keywords = {a version of abstract,abstract,abstract model,categorical logic,category theory,in computer science studies,institution theory,model theory that emerged,of software specifica-,the theory of institutions,theory,this paper builds on,universal logic},
pages = {113--133},
title = {{What is a Logic?}},
url = {https://goo.gl/i3CdWX},
year = {2005}
}
@book{Curry1963,
author = {Curry, Haskell B.},
booktitle = {Journal of the Franklin Institute},
doi = {10.1016/0016-0032(63)90672-0},
isbn = {0486634620},
issn = {00160032},
number = {5},
pages = {438},
pmid = {21192365},
title = {{Foundations of mathematical logic}},
volume = {275},
year = {1963}
}
@misc{Pitts1991,
author = {Pitts, Andrew},
title = {{Notes on Categorical Logic}},
year = {1991}
}
@article{Zalta1995,
author = {Zalta, Edward N and Zalta, Edward N},
journal = {Society},
pages = {1--91},
title = {{Basic Concepts in Modal Logic}},
year = {1995}
}
@article{Blackburn2005,
author = {Blackburn, Patrick and Benthem, Johan Van},
pages = {1--50},
title = {{Modal Logic: A Semantic Perspective}},
url = {papers2://publication/uuid/CC567B68-4635-4E4C-A795-06E2E0B6C800},
year = {2005}
}
@article{VanBenthem2010,
abstract = {We show how belief revision can be treated systematically in the format of dynamic- epistemic logic, when operators of conditional belief are added. The core engine consists of de{\{}{\}}nable update rules for changing plausibility relations between worlds, which have been proposed independently in the dynamic-epistemic literature on preference change. Our analysis yields two new types of modal result. First, we obtain complete logics for concrete mechanisms of belief revision, based on compositional reduction axioms. Next, we show how various ab- stract postulates for belief revision can be analyzed by standard modal frame correspondences for model-changing operations.},
author = {van Benthem, Johan},
isbn = {0521527147},
journal = {Chart},
number = {199},
pages = {1--390},
pmid = {15957556},
title = {{Modal Logic for Open Minds}},
url = {http://fenrong.net/teaching/mljvb.pdf},
year = {2010}
}
@book{VanBenthem1990a,
abstract = {This volume presents a panorama of the applications of logical tools and methods in the formal analysis of natural language. since a number of few developments in philosophical logic were originally stimulated by concern arising in the semantic analysis of natural language discourse, the chapters in this volume provide some criteria of evaluation of the applications of work in philosophical logic. in revealing both the adequacies and inadequacies of logical investigations in the semantic structures of natural discourse, these chapters also point the way to future developments in philosophical logic in general and thus close again the circle of inquiry relating logic and language.},
author = {van Benthem, Johan},
booktitle = {Language},
doi = {10.2307/414900},
isbn = {0198537816},
issn = {00326585},
number = {2},
pages = {555},
title = {{Handbook of Philosophical Logic, Vol. IV: Topics in the Philosophy of Language}},
volume = {66},
year = {1990}
}
@article{Stump2013,
abstract = {Producing and checking proofs from {\{}SMT{\}} solvers is currently the most feasible method for achieving high confidence in the correctness of solver results. The diversity of solvers and relative complexity of {\{}SMT{\}} over, say, {\{}SAT{\}} means that flexibility, as well as performance, is a critical characteristic of a proof-checking solution for {\{}SMT{\}}. This paper describes such a solution, based on a Logical Framework with Side Conditions ({\{}LFSC{\}}). We describe the framework and show how it can be applied for flexible proof production and checking for two different {\{}SMT{\}} solvers, clsat and cvc3. We also report empirical results showing good performance relative to solver execution time.},
author = {Stump, Aaron and Oe, Duckki and Reynolds, Andrew and Hadarean, Liana and Tinelli, Cesare},
doi = {10.1007/s10703-012-0163-3},
issn = {09259856},
journal = {Formal Methods in System Design},
keywords = {Edinburgh logical framework,LFSC,Proof checking,Satisfiability modulo theories},
number = {1},
pages = {91--118},
title = {{SMT proof checking using a logical framework}},
volume = {42},
year = {2013}
}
@book{Chlipala2017,
author = {Chlipala, Adam},
booktitle = {MIT Press},
isbn = {9780262317863},
title = {{Certified Programming with Dependent Types}},
url = {http://adam.chlipala.net/cpdt/cpdt.pdf},
year = {2017}
}
@techreport{Fleury2014,
abstract = {Sledgehammer is a powerful interface from Isabelle to automated provers, to discharge subgoals that appear during the interactive proofs. It chooses facts related to this goal and asks some automatic provers to find a proof. The proof can be either reconstructed or just used to extract the relevant lemmas: in both cases the proof is not trusted. We extend the support by adding one first-order prover (Zipperposition), the reconstruction for two higher-order ATPs (Leo-II and Satallax) and an SMT solver veriT. The support of higher-order prover should especially improve Sledgehammer's performance for higher-order goals. Acknowledgement I would like to thank Jasmin Blanchette for the internship about a very interesting subject, and the members of the logic and verification chair for the welcome. Then Simon Cruanes and Pascal Fontaine (developer of Zipperposition and veriT) were very helpful and provided many explanations concerning their provers and bug fixes.},
author = {Fleury, Mathias and Blanchette, Jasmin},
institution = {Techniche Universit{\"{a}}t M{\"{u}}nchen},
keywords = {Isabelle,Leo-II,Satallax,Sledgehammer,TSTP proof,Zipperposition,higher-order proofs,proof reconstruc-tion,veriT},
pages = {2--0},
title = {{Translation of Proofs Provided by External Provers More Automatic Prover Support for Isabelle: Two Higher-Order Provers and a SMT Solver}},
url = {https://goo.gl/Kw6wWR},
year = {2014}
}
@article{Blanchette2016,
author = {Blanchette, Jasmin and B{\"{o}}hme, Sascha and Fleury, Mathias and Smolka, Steffen Juilf and Steckermeier, Albert},
doi = {10.1007/s10817-015-9335-3},
issn = {15730670},
journal = {Journal of Automated Reasoning},
keywords = {Automatic theorem provers,Natural deduction,Proof assistants},
number = {2},
pages = {155--200},
publisher = {Journal of Automated Reasoning},
title = {{Semi-intelligible Isar Proofs from Machine-Generated Proofs}},
url = {http://dx.doi.org/10.1007/s10817-015-9335-3},
volume = {56},
year = {2016}
}
@article{VanDerWalt2013,
abstract = {This paper explores the recent addition to Agda enablin greflection,in the style of Lisp and Template Haskell. It gives a brief introduction to using reflection, and details the complexities encountered when automating certain proofs with proof by reflection. It presents a library that can be used for automatically quoting a class of concrete Agda terms to a non-dependent, user-defined inductive data type, alleviating some of the burden a programmer faces when using reflection in a practical setting.},
author = {{Van Der Walt}, Paul and Swierstra, Wouter},
doi = {10.1007/978-3-642-41582-1_10},
isbn = {9783642415814},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Agda,Dependently-typed programming,Metaprogramming,Proof by reflection,Reflection},
pages = {157--173},
title = {{Engineering proof by reflection in Agda}},
volume = {8241 LNCS},
year = {2013}
}
@phdthesis{Isaza2014,
author = {Isaza, Juan Pedro Villa},
booktitle = {Eafit.Edu.Co},
keywords = {Category Theory,Haskell},
mendeley-tags = {Category Theory,Haskell},
school = {Universidad EAFIT},
title = {{Category Theory Applied to Functional Programming}},
url = {http://www1.eafit.edu.co/asicard/pubs/cain-screen.pdf},
year = {2014}
}
@article{Norell2009,
abstract = {Dependently typed languages have for a long time been used to describe proofs about programs. Traditionally, dependent types are used mostly for stating and proving the properties of the programs and not in defining the programs themselves. An impressive example is the certified compiler by Leroy (2006) implemented and proved correct in Coq (Bertot and Cast{\'{e}}ran 2004). Recently there has been an increased interest in dependently typed programming, where the aim is to write programs that use the dependent type system to a much higher degree. In this way a lot of the properties that were previously proved separately can be integrated in the type of the program, in many cases adding little or no complexity to the definition of the program. New languages, such as Epigram (McBride and McKinna 2004), are being designed, and existing languages are being extended with new features to accomodate these ideas, for instance the work on dependently typed programming in Coq by Sozeau (2007). This talk gives an overview of the Agda programming language (Norell 2007), whose main focus is on dependently typed programming. Agda provides a rich set of inductive types with a powerful mechanism for pattern matching, allowing dependently typed programs to be written with minimal fuss. To read about programming in Agda, see the lecture notes from the Advanced Functional Programming summer school (Norell 2008) and the work by Oury and Swierstra (2008). In the talk a number of examples of interesting dependently typed programs chosen from the domain of programming language implementation are presented as they are implemented in Agda.},
author = {Norell, Ulf},
doi = {10.1007/978-3-642-04652-0_5},
isbn = {3642046517},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {Agda 2},
pages = {230--266},
title = {{Dependently typed programming in agda}},
volume = {5832 LNCS},
year = {2009}
}
@inproceedings{Farber2015,
abstract = {Metis is an automated theorem prover based on ordered paramodulation. It is widely employed in the interactive theorem provers Isabelle/HOL and HOL4 to automate proofs as well as reconstruct proofs found by automated provers. For both these purposes, the tableaux-based MESON tactic is frequently used in HOL Light. However, paramodulation-based provers such as Metis perform better on many problems involving equality. We created a Metis-based tactic in HOL Light which translates HOL problems to Metis, runs an OCaml version of Metis, and reconstructs proofs in Metis' paramodulation calculus as HOL proofs. We evaluate the performance of Metis as proof reconstruction method in HOL Light. 1},
author = {F{\"{a}}rber, Michael and Kaliszyk, Cezary},
booktitle = {GCAI 2015. Global Conference on Artificial Intelligence Metis-based},
pages = {127--136},
title = {{Metis-based Paramodulation Tactic for HOL Light}},
volume = {36},
year = {2015}
}
@book{TobiasNipkow2016,
author = {Nipkow, Tobias and Klein, Gerwin},
doi = {10.1007/978-3-319-10542-0},
isbn = {9783319105413},
pages = {1--2},
title = {{Concrete Semantics}},
year = {2016}
}
@article{Wadler1993,
abstract = {This tutorial paper provides an introduction to intuitionistic logic and linear logic, and shows how they correspond to type systems for functional languages via the notion of `Propositions as Types'. The presentation of linear logic is simplified by basing it on the Logic of Unity.},
author = {Wadler, Philip},
doi = {10.1007/3-540-57182-5_12},
isbn = {3-540-57182-5},
journal = {Lecture Notes in Computer Science},
number = {September},
pages = {185--210},
title = {{A taste of linear logic}},
url = {http://www.springerlink.com/index/p555h611321h7240.pdf%5Cnhttp://link.springer.com/10.1007/3-540-57182-5_12},
volume = {711},
year = {1993}
}
@book{Jackson2006,
abstract = {{\{}In Software Abstractions Daniel Jackson introduces a new approach to software design that draws on traditional formal methods but exploits automated tools to find flaws as early as possible. This approach--which Jackson calls "lightweight formal methods" or "agile modeling"--takes from formal specification the idea of a precise and expressive notation based on a tiny core of simple and robust concepts but replaces conventional analysis based on theorem proving with a fully automated analysis that gives designers immediate feedback. Jackson has developed Alloy, a language that captures the essence of software abstractions simply and succinctly, using a minimal toolkit of mathematical notions. The designer can use automated analysis not only to correct errors but also to make models that are more precise and elegant. This approach, Jackson says, can rescue designers from "the tarpit of implementation technologies" and return them to thinking deeply about underlying concepts. Software Abstractions introduces the key elements of the approach: a logic, which provides the building blocks of the language; a language, which adds a small amount of syntax to the logic for structuring descriptions; and an analysis, a form of constraint solving that offers both simulation (generating sample states and executions) and checking (finding counterexamples to claimed properties). The book uses Alloy as a vehicle because of its simplicity and tool support, but the book's lessons are mostly language-independent, and could also be applied in the context of other modeling languages.{\}}},
author = {Jackson, Daniel},
booktitle = {MIT Press},
number = {02},
pages = {253},
title = {{Software Abstractions}},
volume = {19},
year = {2006}
}
@book{Baier2008,
abstract = {Our growing dependence on increasingly complex computer and software systems necessitates the development of formalisms, techniques, and tools for assessing functional properties of these systems. One such technique that has emerged in the last twenty years is model checking, which systematically (and automatically) checks whether a model of a given system satisfies a desired property such as deadlock freedom, invariants, or request-response properties. This automated technique for verification and debugging has developed into a mature and widely used approach with many applications. Principles of Model Checking offers a comprehensive introduction to model checking that is not only a text suitable for classroom use but also a valuable reference for researchers and practitioners in the field. The book begins with the basic principles for modeling concurrent and communicating systems, introduces different classes of properties (including safety and liveness), presents the notion of fairness, and provides automata- based algorithms for these properties. It introduces the temporal logics LTL and CTL, compares them, and covers algorithms for verifying these logics, discussing real-time systems as well as systems subject to random phenomena. Separate chapters treat such efficiency-improving techniques as abstraction and symbolic manipulation. The book includes an extensive set of examples (most of which run through several chapters) and a complete set of basic results accompanied by detailed proofs. Each chapter concludes with a summary, bibliographic notes, and an extensive list of exercises of both practical and theoretical nature.},
author = {Baier, Christel and Katoen, Joost-Pieter},
booktitle = {MIT Press},
doi = {10.1093/comjnl/bxp025},
isbn = {9780262026499},
issn = {00155713},
pages = {I--XVII, 1--975},
pmid = {11275744},
title = {{Principles Of Model Checking}},
url = {http://mitpress.mit.edu/books/principles-model-checking},
volume = {950},
year = {2008}
}
@inproceedings{Bohme2011,
abstract = {Automatic provers that can produce proof certificates do not need to be trusted. The certificate can be checked by an independent tool, for example an LCF-style proof assistant such as IsabelleHOL or HOL. Currently, the design of proof formats is mostly dictated by internal constraints of automatic provers and less guided by applications such as checking of certificates. In the worst case, checking can be as involved as the actual proof search simply because important information is missing in the proof certificate. To address this and other issues, we describe design choices for proof formats that we consider both feasible for implementors of automatic provers as well as effective to simplify checking of certificates.},
author = {B{\"{o}}hme, Sascha and Weber, Tjark},
booktitle = {First International Workshop on Proof eXchange for Theorem Proving - PxTP 2011},
title = {{Designing Proof Formats: A User's Perspective - Experience Report}},
url = {http://hal.inria.fr/hal-00677244},
year = {2011}
}
@article{Altenkirch2015,
abstract = {A computer formalisation of the completeness of the boolean model of classical propositional logic is presented. The work follows Huth and Ryan's proof [9]. The proof is constructed for a classical logic system in natural deduction style with all logical connectives. The formalisation is constructive and uses the interactive theorem prover Agda which is an implementation of intensional Martin-L{\"{o}}f type theory [11]. Functions have to be defined in a structurally recursive way to pass the termination checker of Agda. The basic definitions of the formal system must be carefully chosen in order to provide a convenient environment to build correct proofs and meanwhile prevent from getting warnings from the type checker. The formalisation is written in an accessible way so that it can be used for educational purposes. The full source code is available online1. Keywords:},
author = {Cai, Leran and Kaposi, Ambrus and Altenkirch, Thorsten},
title = {{Formalising the Completeness Theorem of Classical Propositional Logic in Agda}},
url = {https://akaposi.github.io/proplogic.pdf},
year = {2015}
}
@inproceedings{DeMoura2008,
abstract = {Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.},
author = {{De Moura}, Leonardo and Bj{\o}rner, Nikolaj},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-78800-3_24},
isbn = {3540787992},
issn = {03029743},
pages = {337--340},
title = {{Z3: An efficient SMT Solver}},
volume = {4963 LNCS},
year = {2008}
}
@article{kanso2016light,
author = {Kanso, Karim and Setzer, Anton},
doi = {10.1017/S0960129514000140},
issn = {09601295},
journal = {Mathematical Structures in Computer Science},
number = {1},
pages = {129--153},
publisher = {Cambridge University Press},
title = {{A Light-weight Integration Of Automated And Interactive Theorem Proving}},
url = {http://cs.swansea.ac.uk/~cskarim/agda/},
volume = {26},
year = {2016}
}
@inproceedings{Weber2006,
abstract = {This paper describes the integration of a leading SAT solver with Isabelle/HOL, a popular interactive theorem prover. The SAT solver generates resolution-style proofs for (instances of) propositional tautologies. These proofs are verified by the theorem prover. The presented approach significantly improves Isabelle's performance on propositional problems, and furthermore exhibits counterexamples for unprovable conjectures. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Weber, Tjark},
booktitle = {Electronic Notes in Theoretical Computer Science},
doi = {10.1016/j.entcs.2005.12.007},
issn = {15710661},
keywords = {LCF-style theorem prover,Proof checking,Propositional resolution,SAT solver},
number = {2 SPEC. ISS.},
pages = {67--78},
title = {{Integrating a SAT solver with an LCF-style theorem prover}},
volume = {144},
year = {2006}
}
@article{Fontaine2006,
abstract = {Formal system development needs expressive specification languages, but also calls for highly automated tools. These two goals are not easy to reconcile, especially if one also aims at high assurances for correctness. In this paper, we describe a combination of {\{}Isabelle/HOL{\}} with a proof-producing {\{}SMT{\}} (Satisfiability Modulo Theories) solver that contains a {\{}SAT{\}} engine and a decision procedure for quantifier-free first-order logic with equality. As a result, a user benefits from the expressiveness of {\{}Isabelle/HOL{\}} when modeling a system, but obtains much better automation for those fragments of the proofs that fall within the scope of the (automatic) {\{}SMT{\}} solver. Soundness is not compromised because all proofs are submitted to the trusted kernel of Isabelle for certification. This architecture is straightforward to extend for other interactive proof assistants and proof-producing reasoners.},
author = {Fontaine, Pascal and Marion, Jean Yves and Merz, Stephan and Nieto, Leonor Prensa and Tiu, Alwen},
doi = {10.1007/11691372_11},
isbn = {3540330569},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {167--181},
title = {{Expressiveness + automation + soundness: Towards combining SMT solvers and interactive proof assistants}},
volume = {3920 LNCS},
year = {2006}
}
@incollection{Riazanov1999,
abstract = {Vampire is a resolution-based theorem prover for rst-order classical logic. The current version implements ordered binary resolution with the set-of-support strategy and ordered hyperresolution. The competition version will have equality rules.},
address = {Berlin, Heidelberg},
author = {Riazanov, Alexandre and Voronkov, Andrei},
booktitle = {Automated Deduction --- CADE-16: 16th International Conference on Automated Deduction Trento, Italy, July 7--10, 1999 Proceedings},
doi = {10.1007/3-540-48660-7_26},
isbn = {978-3-540-48660-2},
pages = {292--296},
publisher = {Springer Berlin Heidelberg},
title = {{Vampire}},
url = {https://doi.org/10.1007/3-540-48660-7_26},
year = {1999}
}
@article{Farber2016,
abstract = {Copyright ? by the paper's authors.Proof assistants based on higher-order logic frequently use first-order automated theorem provers as proof search mechanisms. The reconstruction of the proofs generated by common tools, such as MESON and Metis, typically involves the use of the axiom of choice to simulate the Skolemisation steps. In this paper we present a method to reconstruct the proofs without introducing Skolem functions. This enables us to integrate tactics that use first-order automated theorem provers in logics that feature neither the axiom of choice nor the definite description operator.},
author = {F{\"{a}}rber, Michael and Kaliszyk, Cezary},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
number = {Paar},
pages = {24--31},
title = {{No choice: Reconstruction of first-order ATP proofs without skolem functions}},
url = {https://goo.gl/V9dsoH},
volume = {1635},
year = {2016}
}
@article{hurd2003first,
abstract = {In this paper we evaluate the effectiveness of first-order proof procedures when used as tactics for proving subgoals in a higher-order logic interactive theorem prover. We first motivate why such first-order proof tactics are useful, and then describe the core integrating technology: an `LCF-style' logical kernel for clausal first-order logic. This allows the choice of different logical mappings between higher-order logic and first-order logic to be used depending on the subgoal, and also enables several different first-order proof procedures to cooperate on constructing the proof. This work was carried out using the HOL4 theorem prover; we comment on the ease of transferring the technology to other higher-order logic theorem provers},
author = {Hurd, Joe},
journal = {Design and Application of Strategies/Tactics in Higher Order Logics, number NASA/CP-2003-212448 in NASA Technical Reports},
pages = {56--68},
title = {{First-order Proof Tactics In Higher-order Logic Theorem Provers}},
url = {http://www.gilith.com/research/papers},
year = {2003}
}
@inproceedings{paulson2010three,
author = {Paulson, Lawrence C. and Blanchette, Jasmin},
booktitle = {PAAR@ IJCAR},
pages = {1--10},
title = {{Three Years Of Experience with Sledgehammer, A Practical Link Between Automatic And Interactive Theorem Provers.}},
year = {2010}
}
@incollection{Barrett2011,
abstract = {CVC4 is the latest version of the Cooperating Validity Checker. A joint project of NYU and U Iowa, CVC4 aims to support the useful feature set of CVC3 and SMT-LIBv2 while optimizing the design of the core system architecture and decision procedures to take advantage of recent engineering and algorithmic advances. CVC4 represents a completely new code base; it is a from-scratch rewrite of CVC3, and many subsystems have been completely redesigned. Additional decision procedures for CVC4 are currently under development, but for what it currently achieves, it is a lighter-weight and higher-performing tool than CVC3. We describe the system architecture, subsystems of note, and discuss some applications and continuing work.},
address = {Berlin, Heidelberg},
author = {Barrett, Clark and Conway, Christopher L. and Deters, Morgan and Hadarean, Liana and Jovanovi{\'{c}}, Dejan and King, Tim and Reynolds, Andrew and Tinelli, Cesare},
booktitle = {Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings},
doi = {10.1007/978-3-642-22110-1_14},
editor = {Gopalakrishnan, Ganesh and Qadeer, Shaz},
isbn = {978-3-642-22110-1},
pages = {171--177},
publisher = {Springer Berlin Heidelberg},
title = {{CVC4}},
year = {2011}
}
@inproceedings{paulson2007source,
author = {Paulson, Lawrence C. and Susanto, Kong Woei},
booktitle = {TPHOLs},
doi = {https://doi.org/10.1007/978-3-540-74591-4_18},
organization = {Springer},
pages = {232--245},
title = {{Source-level Proof Reconstruction For Interactive Theorem Proving}},
volume = {4732},
year = {2007}
}
@misc{denivelle2003,
author = {{De Nivelle}, Hans},
title = {{Bliksem 1.10 User Manual}},
url = {https://goo.gl/ZZjaZW}
}
@incollection{armand2011,
abstract = {We present a way to enjoy the power of SAT and SMT provers in Coq without compromising soundness. This requires these provers to return not only a yes/no answer, but also a proof witness that can be independently rechecked. We present such a checker, written and fully certified in Coq. It is conceived in a modular way, in order to tame the proofs' complexity and to be extendable. It can currently check witnesses from the SAT solver ZChaff and from the SMT solver veriT. Experiments highlight the efficiency of this checker. On top of it, new reflexive Coq tactics have been built that can decide a subset of Coq's logic by calling external provers and carefully checking their answers.},
address = {Berlin, Heidelberg},
author = {Armand, Michael and Faure, Germain and Gr{\'{e}}goire, Benjamin and Keller, Chantal and Th{\'{e}}ry, Laurent and Werner, Benjamin},
booktitle = {Certified Programs and Proofs: First International Conference, CPP 2011, Kenting, Taiwan, December 7-9, 2011. Proceedings},
doi = {10.1007/978-3-642-25379-9_12},
isbn = {978-3-642-25379-9},
pages = {135--150},
publisher = {Springer Berlin Heidelberg},
title = {{A Modular Integration of SAT/SMT Solvers to Coq through Proof Witnesses}},
url = {https://doi.org/10.1007/978-3-642-25379-9_12},
year = {2011}
}
@incollection{bouton2009,
abstract = {This article describes the first public version of the satisfiability modulo theory (SMT) solver veriT. It is open-source, proof-producing, and complete for quantifier-free formulas with uninterpreted functions and difference logic on real numbers and integers.},
address = {Berlin, Heidelberg},
author = {Bouton, Thomas and de Oliveira, Diego and D{\'{e}}harbe, David and Fontaine, Pascal},
booktitle = {Automated Deduction -- CADE-22: 22nd International Conference on Automated Deduction, Montreal, Canada, August 2-7, 2009. Proceedings},
doi = {10.1007/978-3-642-02959-2_12},
isbn = {978-3-642-02959-2},
pages = {151--156},
publisher = {Springer Berlin Heidelberg},
title = {{veriT: An Open, Trustable and Efficient SMT-Solver}},
year = {2009}
}
@book{humberstone2011,
author = {Humberstone, L.},
publisher = {MIT Press},
title = {{The Connectives}},
year = {2011}
}
@book{paulson1994isabelle,
author = {Paulson, Lawrence C.},
publisher = {Springer Science {\&} Business Media},
title = {{Isabelle: A Generic Theorem Prover}},
volume = {828},
year = {1994}
}
@article{appel1959,
author = {Appel, K. I.},
issn = {00224812},
journal = {The Journal of Symbolic Logic},
number = {4},
pages = {306--310},
publisher = {Association for Symbolic Logic},
title = {{Horn Sentences in Identity Theory}},
url = {http://www.jstor.org/stable/2963901},
volume = {24},
year = {1959}
}
@article{meng2006automation,
author = {Meng, Jia and Quigley, Claire and Paulson, Lawrence C.},
journal = {Information and computation},
number = {10},
pages = {1575--1596},
publisher = {Elsevier},
title = {{Automation for Interactive Proof: First Prototype}},
volume = {204},
year = {2006}
}
@book{nipkow2002isabelle,
author = {Nipkow, Tobias and Paulson, Lawrence C. and Wenzel, Markus},
publisher = {Springer Science {\&} Business Media},
title = {{Isabelle/HOL: A Proof Assistant for Higher-order Logic}},
volume = {2283},
year = {2002}
}
@article{sutcliffe2009,
abstract = {This paper describes the First-Order Form (FOF) and Clause Normal Form (CNF) parts of the TPTP problem library, and the associated infrastructure. TPTP v3.5.0 was the last release containing only FOF and CNF problems, and thus serves as the exemplar. This paper summarizes the history and development of the TPTP, describes the structure and contents of the TPTP, and gives an overview of TPTP related projects and tools.},
author = {Sutcliffe, Geoff},
doi = {10.1007/s10817-009-9143-8},
issn = {1573-0670},
journal = {Journal of Automated Reasoning},
number = {4},
pages = {337},
title = {{The TPTP Problem Library and Associated Infrastructure}},
url = {https://doi.org/10.1007/s10817-009-9143-8},
volume = {43},
year = {2009}
}
@incollection{kaliszyk2013,
abstract = {PRocH is a proof reconstruction tool that imports in HOL Light proofs produced by ATPs on the recently developed translation of HOL Light and Flyspeck problems to ATP formats. PRocH combines several reconstruction methods in parallel, but the core improvement over previous methods is obtained by re-playing in the HOL logic the detailed inference steps recorded in the ATP (TPTP) proofs, using several internal HOL Light inference methods. These methods range from fast variable matching and more involved rewriting, to full first-order theorem proving using the MESON tactic. The system is described and its performance is evaluated here on a large set of Flyspeck problems.},
address = {Berlin, Heidelberg},
author = {Kaliszyk, Cezary and Urban, Josef},
booktitle = {Automated Deduction -- CADE-24: 24th International Conference on Automated Deduction, Lake Placid, NY, USA, June 9-14, 2013. Proceedings},
doi = {10.1007/978-3-642-38574-2_18},
editor = {Bonacina, Maria Paola},
isbn = {978-3-642-38574-2},
pages = {267--274},
publisher = {Springer Berlin Heidelberg},
title = {{PRocH: Proof Reconstruction for HOL Light}},
url = {https://doi.org/10.1007/978-3-642-38574-2_18},
year = {2013}
}
@incollection{sultana2015,
abstract = {Implementing proof reconstruction is difficult because it involves symbolic manipulations of formal objects whose representation varies between different systems. It requires significant knowledge of the source and target systems. One cannot simply re-target to another logic. We present a modular proof reconstruction system with separate components, specifying their behaviour and describing how they interact. This system is demonstrated and evaluated through an implementation to reconstruct proofs generated by Leo-II and Satallax in Isabelle HOL, and is shown to work better than the current method of rediscovering proofs using a select set of provers.},
address = {Cham},
author = {Sultana, Nik and Benzm{\"{u}}ller, Christoph and Paulson, Lawrence C.},
booktitle = {Frontiers of Combining Systems: 10th International Symposium, FroCoS 2015, Wroclaw, Poland, September 21-24, 2015, Proceedings},
doi = {10.1007/978-3-319-24246-0_16},
editor = {Lutz, Carsten and Ranise, Silvio},
isbn = {978-3-319-24246-0},
pages = {256--271},
publisher = {Springer International Publishing},
title = {{Proofs and Reconstructions}},
url = {https://doi.org/10.1007/978-3-319-24246-0_16},
year = {2015}
}
@inproceedings{hurlin07practical,
address = {Bremen, Germany},
author = {Hurlin, Cl{\'}ment and Chaieb, Amine and Fontaine, Pascal and Merz, Stephan and Weber, Tjark},
booktitle = {Proceedings of the Isabelle Workshop 2007},
editor = {Dixon, Lucas and Johansson, Moa},
pages = {2--13},
title = {{Practical Proof Reconstruction for First-Order Logic and Set-Theoretical Constructions}},
year = {2007}
}
@incollection{foster2011integrating,
abstract = {Agda is a dependently typed functional programming language and a proof assistant in which developing programs and proving their correctness is one activity. We show how this process can be enhanced by integrating external automated theorem provers, provide a prototypical integration of the equational theorem prover Waldmeister, and give examples of how this proof automation works in practice.},
address = {Berlin, Heidelberg},
author = {Foster, Simon and Struth, Georg},
booktitle = {NASA Formal Methods: Third International Symposium, NFM 2011, Pasadena, CA, USA, April 18-20, 2011. Proceedings},
doi = {10.1007/978-3-642-20398-5_10},
isbn = {978-3-642-20398-5},
pages = {116--130},
publisher = {Springer Berlin Heidelberg},
title = {{Integrating an Automated Theorem Prover into Agda}},
year = {2011}
}
@article{blanchette2013extending,
abstract = {Sledgehammer is a component of Isabelle/HOL that employs resolution-based first-order automatic theorem provers (ATPs) to discharge goals arising in interactive proofs. It heuristically selects relevant facts and, if an ATP is successful, produces a snippet that replays the proof in Isabelle. We extended Sledgehammer to invoke satisfiability modulo theories (SMT) solvers as well, exploiting its relevance filter and parallel architecture. The ATPs and SMT solvers nicely complement each other, and Isabelle users are now pleasantly surprised by SMT proofs for problems beyond the ATPs' reach.},
author = {Blanchette, Jasmin and B{\"{o}}hme, Sascha and Paulson, Lawrence C.},
doi = {10.1007/s10817-013-9278-5},
issn = {1573-0670},
journal = {Journal of Automated Reasoning},
number = {1},
pages = {109--128},
title = {{Extending Sledgehammer with SMT Solvers}},
url = {https://doi.org/10.1007/s10817-013-9278-5},
volume = {51},
year = {2013}
}
@incollection{bohme2010,
abstract = {The Satisfiability Modulo Theories (SMT) solver Z3 can generate proofs of unsatisfiability. We present independent reconstruction of these proofs in the theorem provers Isabelle/HOL and HOL4 with particular focus on efficiency. Our highly optimized implementations outperform previous LCF-style proof checkers for SMT, often by orders of magnitude. Detailed performance data shows that LCF-style proof reconstruction can be faster than proof search in Z3.},
address = {Berlin, Heidelberg},
author = {B{\"{o}}hme, Sascha and Weber, Tjark},
booktitle = {Interactive Theorem Proving: First International Conference, ITP 2010, Edinburgh, UK, July 11-14, 2010. Proceedings},
doi = {10.1007/978-3-642-14052-5_14},
editor = {Kaufmann, Matt and Paulson, Lawrence C.},
isbn = {978-3-642-14052-5},
pages = {179--194},
publisher = {Springer Berlin Heidelberg},
title = {{Fast LCF-Style Proof Reconstruction for Z3}},
year = {2010}
}
@misc{norrish2017hol,
author = {Norrish, Michael and Slind, Konrad},
title = {{The HOL system description}},
url = {https://goo.gl/iG8b9M},
year = {2017}
}
@misc{agdateam,
author = {Team, The Agda Developement},
edition = {Version 8.},
keywords = {agda},
mendeley-tags = {agda},
title = {{Agda 2.4.2.3}},
url = {http://wiki.portal.chalmers.se/agda/pmwiki.php},
year = {2015}
}
@article{Schulz:AICOM-2002,
annote = {StS},
author = {Schulz, Stephan},
journal = {Journal of AI Communications},
number = {2/3},
pages = {111--126},
title = {{E -- A Brainiac Theorem Prover}},
volume = {15},
year = {2002}
}
@article{sutcliffe2004tstp,
author = {Sutcliffe, Geoff and Zimmer, J{\"{u}}rgen and Schulz, Stephan},
journal = {Distributed Constraint Problem Solving and Reasoning in Multi-Agent Systems},
pages = {201--215},
title = {{TSTP data-exchange formats for automated theorem proving tools}},
volume = {112},
year = {2004}
}
@misc{coqteam,
author = {Team, The Coq Developement},
edition = {Version 8.},
title = {{The Coq Proof Assistant. Reference Manual}},
year = {2015}
}
@inproceedings{Sut07-CSR,
author = {Sutcliffe, Geoff},
booktitle = {Proceedings of the 2nd International Computer Science Symposium in Russia},
editor = {Diekert, V and Volkov, M and Voronkov, A},
number = {4649},
pages = {7--23},
publisher = {Springer-Verlag},
series = {Lecture Notes in Computer Science},
title = {{TPTP, TSTP, CASC, etc.}},
year = {2007}
}
@article{hillenbrand1997,
abstract = {Waldmeister is a high-performance theorem prover for unit equational first-order logic. In the making of Waldmeister, we have applied an engineering approach, identifying the critical points with respect to efficiency in time and space. Our logical three-level system model consists of the basic operations on the lowest level, where we put great stress on efficient data structures and algorithms. For the middle level, where the inference steps are aggregated into an inference machine, flexible adjustment has proven essential during experimental evaluation. The top level holds control strategy and reduction ordering. Although at this level only standard strategies are employed, really large proof tasks have been managed in reasonable time.},
author = {Hillenbrand, Thomas and Buch, Arnim and Vogt, Roland and L{\"{o}}chner, Bernd},
doi = {10.1023/A:1005872405899},
issn = {1573-0670},
journal = {Journal of Automated Reasoning},
number = {2},
pages = {265--270},
title = {{WALDMEISTER - High-Performance Equational Deduction}},
url = {https://doi.org/10.1023/A:1005872405899},
volume = {18},
year = {1997}
}
@article{Weber2009,
abstract = {This paper describes the integration of zChaff and MiniSat, currently two leading SAT solvers, with Higher Order Logic (HOL) theorem provers. Both SAT solvers generate resolution-style proofs for (instances of) propositional tautologies. These proofs are verified by the theorem provers. The presented approach significantly improves the provers' performance on propositional problems, and exhibits counterexamples for unprovable conjectures. It is also shown that LCF-style theorem provers can serve as viable proof checkers even for large SAT problems. An efficient representation of the propositional problem in the theorem prover turns out to be crucial; several possible solutions are discussed. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Weber, Tjark and Amjad, Hasan},
doi = {10.1016/j.jal.2007.07.003},
issn = {15708683},
journal = {Journal of Applied Logic},
keywords = {Interactive theorem proving,LCF-style proof checking,Propositional resolution},
number = {1},
pages = {26--40},
title = {{Efficiently checking propositional refutations in HOL theorem provers}},
volume = {7},
year = {2009}
}
@misc{Hurd1999,
author = {Hurd, Joe},
doi = {10.1007/3-540-48256-3_21},
pages = {311--321},
publisher = {Springer, Berlin, Heidelberg},
title = {{Integrating Gandalf and HOL}},
url = {http://link.springer.com/10.1007/3-540-48256-3_21},
year = {1999}
}
@inproceedings{Weidenbach2009,
author = {Weidenbach, Christoph and Dimova, Dilyana and Fietzke, Arnaud and Kumar, Rohit and Suda, Martin and Wischnewski, Patrick},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-02959-2_10},
isbn = {3642029582},
issn = {03029743},
pages = {140--145},
title = {{SPASS version 3.5}},
volume = {5663 LNAI},
year = {2009}
}
